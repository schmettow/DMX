@article{Kroenke2001,
   author = {Kurt Kroenke and Robert L. Spitzer and Janet B. W. Williams},
   doi = {10.1046/j.1525-1497.2001.016009606.x},
   issn = {0884-8734},
   issue = {9},
   journal = {Journal of General Internal Medicine},
   month = {9},
   pages = {606-613},
   title = {The PHQ-9},
   volume = {16},
   year = {2001}
}

@article{Fabrigar1999,
   abstract = {Despite the widespread use of exploratory factor analysis in psychological research, researchers often make questionable decisions when conducting these analyses. This article reviews the major design and analytical decisions that must be made when conducting a factor analysis and notes that each of these decisions has important consequences for the obtained results. Recommendations that have been made in the methodological literature are discussed. Analyses of 3 existing empirical data sets are used to illustrate how questionable decisions in conducting factor analyses can yield problematic results. The article presents a survey of 2 prominent journals that suggests that researchers routinely conduct analyses using such questionable methods. The implications of these practices for psychological research are discussed, and the reasons for current practices are reviewed.},
   author = {Leandre R Fabrigar and Duane T Wegener and Robert C Maccallum and Erin J Strahan},
   issue = {3},
   journal = {Psychological Methods},
   pages = {272-299},
   title = {Evaluating the Use of Exploratory Factor Analysis in Psychological Research},
   volume = {4},
   year = {1999}
}

@article{Clark1995,
   author = {Lee Anna Clark and David Watson},
   issue = {3},
   journal = {Psychological Assessment},
   title = {Scale-validity},
   volume = {7},
   year = {1995}
}

@article{Peterson1994,
   author = {Robert A Peterson},
   issue = {2},
   journal = {Source: Journal of Consumer Research},
   month = {9},
   pages = {381-391},
   title = {A Meta-Analysis of Cronbach's Coefficient Alpha},
   volume = {21},
   url = {https://www.jstor.org/stable/2489828},
   year = {1994}
}

@article{Cronbach1951,
   abstract = {A general formula (α) of which a special case is the Kuder-Richardson coefficient of equivalence is shown to be the mean of all split-half coefficients resulting from different splittings of a test. α is therefore an estimate of the correlation between two random samples of items from a universe of items like those in the test. α is found to be an appropriate index of equivalence and, except for very short tests, of the first-factor concentration in the test. Tests divisible into distinct subtests should be so divided before using the formula. The index \{Mathematical expression\}, derived from α, is shown to be an index of inter-item homogeneity. Comparison is made to the Guttman and Loevinger approaches. Parallel split coefficients are shown to be unnecessary for tests of common types. In designing tests, maximum interpretability of scores is obtained by increasing the first-factor concentration in any separately-scored subtest and avoiding substantial group-factor clusters within a subtest. Scalability is not a requisite. © 1951 Psychometric Society.},
   author = {Lee J. Cronbach},
   doi = {10.1007/BF02310555},
   issn = {00333123},
   issue = {3},
   journal = {Psychometrika},
   month = {9},
   pages = {297-334},
   publisher = {Springer-Verlag},
   title = {Coefficient alpha and the internal structure of tests},
   volume = {16},
   year = {1951}
}

@article{Hinkin1998,
   abstract = {The adequate measurement of abstract constructs is perhaps the greatest challenge to understanding the behavior of people in organizations. Problems with the reliability and validity of measures us...},
   author = {Timothy R. Hinkin},
   doi = {10.1177/109442819800100106},
   issn = {10944281},
   issue = {1},
   journal = {Organizational Research Methods},
   pages = {104-121},
   publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
   title = {A Brief Tutorial on the Development of Measures for Use in Survey Questionnaires},
   volume = {1},
   url = {/doi/pdf/10.1177/109442819800100106?download=true},
   year = {1998}
}

@article{Schmidt1998,
   abstract = {This article summarizes the practical and theoretical implications of 85 years of research in personnel selection. On the basis of meta-analytic findings, this article presents the validity of 19 selection procedures for predicting job performance and training performance and the validity of paired combinations of general mental ability (GMA) and Ihe 18 other selection procedures. Overall, the 3 combinations with the highest multivariate validity and utility for job performance were GMA plus a work sample test (mean validity of .63), GMA plus an integrity test (mean validity of .65), and GMA plus a structured interview (mean validity of .63). A further advantage of the latter 2 combinations is that they can be used for both entry level selection and selection of experienced employees. The practical utility implications of these summary findings are substantial. The implications of these research findings for the development of theories of job performance are discussed.},
   author = {Frank L Schmidt and John E Hunter},
   issue = {2},
   journal = {Psychological Bulletin},
   pages = {262-274},
   title = {The Validity and Utility of Selection Methods in Personnel Psychology: Practical and Theoretical Implications of 85 Years of Research Findings},
   volume = {124},
   year = {1998}
}

@article{McAdams1992,
   abstract = { ABSTRACT This critical appraisal aims to position the five‐factor model within the multifaceted field of personality psychology by highlighting six important limitations of the model. These are the model's ( a ) inability to address core constructs of personality functioning beyond the level of traits; ( b ) limitations with respect to the prediction of specific behavior and the adequate description of personsl' lives; ( c ) failure to provide compelling causal explanations for human behavior and experience; ( d ) disregard of the contextual and conditional nature of human experience; ( e ) failure to offer an attractive program for studying personality organization and integration; and ( f ) reliance on simple, noncontingent, and implicitly comparative statements about persons. The five‐factor model is essentially a “psychology of the stranger,” providing information about persons that one would need to know when one knows nothing else about them. It is argued that because of inherent limitations, the Big Five may be viewed as one important model in personality studies but not the integrative model of personality. },
   author = {Dan P. McAdams},
   doi = {10.1111/j.1467-6494.1992.tb00976.x},
   issn = {0022-3506},
   issue = {2},
   journal = {Journal of Personality},
   month = {6},
   pages = {329-361},
   title = {The Five‐Factor Model Personality: A Critical Appraisal},
   volume = {60},
   year = {1992}
}

@article{Wickens2002,
   author = {Christopher D. Wickens},
   doi = {10.1080/14639220210123806},
   issn = {1463-922X},
   issue = {2},
   journal = {Theoretical Issues in Ergonomics Science},
   month = {1},
   pages = {159-177},
   title = {Multiple resources and performance prediction},
   volume = {3},
   year = {2002}
}

@book{Brown2015,
   abstract = {Second edition. "With its emphasis on practical and conceptual aspects, rather than mathematics or formulas, this accessible book has established itself as the go-to resource on confirmatory factor analysis (CFA). Detailed, worked-through examples drawn from psychology, management, and sociology studies illustrate the procedures, pitfalls, and extensions of CFA methodology. The text shows how to formulate, program, and interpret CFA models using popular latent variable software packages (LISREL, Mplus, EQS, SAS/CALIS); understand the similarities and differences between CFA and exploratory factor analysis (EFA); and report results from a CFA study. It is filled with useful advice and tables that outline the procedures. The companion website offers data and program syntax files for most of the research examples, as well as links to CFA-related resources. New to This Edition *Updated throughout to incorporate important developments in latent variable modeling. *Chapter on Bayesian CFA and multilevel measurement models. *Addresses new topics (with examples): exploratory structural equation modeling, bifactor analysis, measurement invariance evaluation with categorical indicators, and a new method for scaling latent variables. *Utilizes the latest versions of major latent variable software packages"-- Provided by publisher. The common factor model and exploratory factor analysis -- Introduction to CFA -- Specification and interpretation of CFA models -- Model revision and comparison -- CFA of multitrait-multimethod matrices -- CFA with equality constraints, multiple groups, and mean structures -- Other types of CFA models : higher-order factor analysis, scale reliability evaluation, and formative indicators -- Data issues in CFA : missing, non-normal, and categorical data -- Statistical power and sample size -- recent developments involving CFA models.},
   author = {Timothy A.. Brown},
   isbn = {146251779X},
   pages = {482},
   publisher = {The Guilford Press},
   title = {Confirmatory factor analysis for applied research},
   year = {2015}
}


@article{Costello2005,
   abstract = {Exploratory factor analysis (EFA) is a complex, multi-step process. The goal of this paper is to collect, in one article, information that will allow researchers and practitioners to understand the various choices available through popular software packages, and to make decisions about "best practices" in exploratory factor analysis. In particular, this paper provides practical information on making decisions regarding (a) extraction, (b) rotation, (c) the number of factors to interpret, and (d) sample size.},
   author = {Anna B Costello and Jason W Osborne},
   doi = {https://doi.org/10.7275/jyj1-4868},
   issn = {1531-7714},
   journal = {Practical Assessment, Research \& Evaluation},
   keywords = {Factor analysis,PCA},
   month = {1},
   title = {Best practices in exploratory factor analysis: four recommendations for getting the most from your analysis.},
   volume = {10},
   url = {http://www.statsoft.com/textbook/},
   year = {2005}
}

@article{Lim2019,
   abstract = {Parallel analysis (PA) is recommended as one of the best procedures to determine the number of factors but its theoretical justification has long been questioned. The current study discussed theoretical issues on the use of eigenvalues for dimensionality assessment and reviewed the development of PA and its recent variants proposed to address the issues. The performances of 13 different PAs including PA with minimum rank factor analysis, revised PA, and comparison data method were investigated through a Monte Carlo simulation under a wide range of factor structures that produce small factor-representing and nonrepresenting eigenvalues for different types of measurement scales. Results showed that the traditional PA using full correlation matrices performed best in most of the conditions, especially when population error was involved. However, the overall accuracy of PA was not satisfactory when factor-representing eigenvalues were small, that is, when factor loadings were low and factor correlations were high. From these results, we suggest that the original PA be used to determine the number of factors but the estimated number should not be taken as a fixed estimate. The number of factors within ± 1 range of the estimate can be considered as viable candidates and interpretational validity of the competing models should be consulted for the decision.},
   author = {Sangdon Lim and Seungmin Jahng},
   doi = {10.1037/met0000230},
   issn = {1939-1463},
   issue = {4},
   journal = {Psychological Methods},
   keywords = {Comparison data method,Minimum rank factor analysis,Number of factors,Parallel analysis,Revised parallel analysis},
   month = {8},
   pages = {452-467},
   pmid = {31180694},
   publisher = {American Psychological Association Inc.},
   title = {Determining the number of factors using parallel analysis and its recent variants.},
   volume = {24},
   url = {https://doi.apa.org/doi/10.1037/met0000230},
   year = {2019}
}


@book{Embretson2013,
   author = {Susan E. Embretson and Steven P. Reise},
   doi = {10.4324/9781410605269},
   isbn = {9781410605269},
   month = {9},
   publisher = {Psychology Press},
   title = {Item Response Theory},
   year = {2013}
}

@inbook{Rost2001,
   author = {Jürgen Rost},
   city = {New York},
   editor = {Anne Boomsma and Marijtje A J van Duijn and Tom A B Snijders},
   booktitle = {Essays on Item Response Theory},
   pages = {25-42},
   publisher = {Springer},
   title = {The Growing family of Rasch Models},
   volume = {157},
   year = {2001}
}

@book{Rasch1960,
   abstract = {This monograph attempts a new approach for testing construction in psychology. A probabilistic model is developed for 3 different types of tests. Each model implies 2 types of parameters: a "difficulty" for each test or item and an "ability" for each person, independent of which set of tests or items has been employed. Both parameters are estimated from the data. Chapters 1-7 present the basic theory with a minimum of mathematics. Chapters 8-10 present in detail the mathematics underlying the models. From Psyc Abstracts 36:05:5HB84R. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
   author = {Georg Rasch},
   city = {Oxford,  England},
   journal = {Studies in mathematical psychology: I. Probabilistic models for some intelligence and attainment tests.},
   pages = {184, xiii, 184-xiii},
   publisher = {Nielsen \& Lydiche},
   title = {Studies in mathematical psychology: I. Probabilistic models for some intelligence and attainment tests.},
   year = {1960}
}

@article{Penfield2000,
   abstract = {How can we best extend DIF research to performance assessment? What are the issues and problems surrounding studies of DIF on complex tasks? What appear to be the best approaches at this time?. © 2017 Wiley. All rights reserved.},
   author = {Randall D. Penfield and Tony C.M. Lam},
   doi = {10.1111/J.1745-3992.2000.TB00033.X},
   issn = {1745-3992},
   issue = {3},
   journal = {Educational Measurement: Issues and Practice},
   month = {9},
   pages = {5-15},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Assessing Differential Item Functioning in Performance Assessment: Review and Recommendations},
   volume = {19},
   url = {/doi/pdf/10.1111/j.1745-3992.2000.tb00033.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-3992.2000.tb00033.x https://onlinelibrary.wiley.com/doi/10.1111/j.1745-3992.2000.tb00033.x},
   year = {2000}
}

@article{Wolf2013,
   abstract = {Determining sample size requirements for structural equation modeling (SEM) is a challenge often faced by investigators, peer reviewers, and grant writers. Recent years have seen a large increase in SEMs in the behavioral science literature, but consideration of sample size requirements for applied SEMs often relies on outdated rules-of-thumb. This study used Monte Carlo data simulation techniques to evaluate sample size requirements for common applied SEMs. Across a series of simulations, we systematically varied key model properties, including number of indicators and factors, magnitude of factor loadings and path coefficients, and amount of missing data. We investigated how changes in these parameters affected sample size requirements with respect to statistical power, bias in the parameter estimates, and overall solution propriety. Results revealed a range of sample size requirements (i.e., from 30 to 460 cases), meaningful patterns of association between parameters and sample size, and highlight the limitations of commonly cited rules-of-thumb. The broad "lessons learned" for determining SEM sample size requirements are discussed. © The Author(s) 2013.},
   author = {Erika J. Wolf and Kelly M. Harrington and Shaunna L. Clark and Mark W. Miller},
   doi = {10.1177/0013164413495237},
   issn = {15523888},
   issue = {6},
   journal = {Educational and Psychological Measurement},
   keywords = {Monte Carlo simulation,bias,confirmatory factor analysis,sample size,solution propriety,statistical power,structural equation modeling},
   pages = {913-934},
   pmid = {25705052},
   publisher = {SAGE Publications Inc.},
   title = {Sample Size Requirements for Structural Equation Models: An Evaluation of Power, Bias, and Solution Propriety},
   volume = {73},
   year = {2013}
}

@article{Anderson1988,
   abstract = {In this article, we provide guidance for substantive researchers on the use of structural equation modeling in practice for theory testing and development. We present a comprehensive, two-step modeling approach that employs a series of nested models and sequential chi-square difference tests. We discuss the comparative advantages of this approach over a one-step approach. Considerations in specification, assessment of fit, and respecification of measurement models using confirmatory factor analysis are reviewed. As background to the two-step approach, the distinction between exploratory and confirmatory analysis, the distinction between complementary approaches for theory testing versus predictive application, and some developments in estimation methods also are discussed .},
   author = {James C Anderson and J L Kellogg and David W Gerbing},
   issue = {3},
   journal = {Psychological Bulletin},
   pages = {411-423},
   title = {Structural Equation Modeling in Practice: A Review and Recommended Two-Step Approach},
   volume = {103},
   year = {1988}
}


@article{Borsci2024,
   abstract = {Intelligent systems, such as chatbots, are likely to strike new qualities of UX that are not covered by instruments validated for legacy human–computer interaction systems. A new validated tool to evaluate the interaction quality of chatbots is the chatBot Usability Scale (BUS) composed of 11 items in five subscales. The BUS-11 was developed mainly from a psychometric perspective, focusing on ranking people by their responses and also by comparing designs’ properties (designometric). In this article, 3186 observations (BUS-11) on 44 chatbots are used to re-evaluate the inventory looking at its factorial structure, and reliability from the psychometric and designometric perspectives. We were able to identify a simpler factor structure of the scale, as previously thought. With the new structure, the psychometric and the designometric perspectives coincide, with good to excellent reliability. Moreover, we provided standardized scores to interpret the outcomes of the scale. We conclude that BUS-11 is a reliable and universal scale, meaning that it can be used to rank people and designs, whatever the purpose of the research.},
   author = {Simone Borsci and Martin Schmettow},
   doi = {10.1007/s00779-024-01834-4},
   issn = {16174917},
   issue = {6},
   journal = {Personal and Ubiquitous Computing},
   keywords = {Chatbot,Conversational agents,Human AI interaction,Satisfaction,Usability},
   month = {12},
   pages = {1033-1044},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Re-examining the chatBot Usability Scale (BUS-11) to assess user experience with customer relationship management chatbots},
   volume = {28},
   year = {2024}
}

@article{Ho2017,
   abstract = {© 2016, Springer Science+Business Media Dordrecht. Using a hypothetical graph, Masahiro Mori proposed in 1970 the relation between the human likeness of robots and other anthropomorphic characters and an observer’s affective or emotional appraisal of them. The relation is positive apart from a U-shaped region known as the uncanny valley. To measure the relation, we previously developed and validated indices for the perceptual-cognitive dimension humanness and three affective dimensions: interpersonal warmth, attractiveness, and eeriness. Nevertheless, the design of these indices was not informed by how the untrained observer perceives anthropomorphic characters categorically. As a result, scatter plots of humanness vs. eeriness show the stimuli cluster tightly into categories widely separated from each other. The present study applies a card sorting task, laddering interview, and adjective evaluation (N= 30) to revise the humanness, attractiveness, and eeriness indices and validate them via a representative survey (N= 1311). The revised eeriness index maintains its orthogonality to humanness (r=. 04 , p=. 285), but the stimuli show much greater spread, reflecting the breadth of their range in human likeness and eeriness. The revised indices enable empirical relations among characters to be plotted similarly to Mori’s graph of the uncanny valley. Accurate measurement with these indices can be used to enhance the design of androids and 3D computer animated characters.},
   author = {Chin Chang Ho and Karl F. MacDorman},
   doi = {10.1007/s12369-016-0380-9},
   issn = {18754805},
   issue = {1},
   journal = {International Journal of Social Robotics},
   keywords = {Anthropomorphism,Categorical perception,Cognitive bias,Psychometric scales,Social perception},
   pages = {129-139},
   publisher = {Springer Netherlands},
   title = {Measuring the Uncanny Valley Effect: Refinements to Indices for Perceived Humanness, Attractiveness, and Eeriness},
   volume = {9},
   year = {2017}
}





