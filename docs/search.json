[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Designometricon",
    "section": "",
    "text": "Preface\nThis book is (becoming) a collection of articles about designometric construction of research instruments.\nTo start with a hard-to-deny statement:\n\n1 + 1\n\n[1] 2\n\n\nThe discovery of designometrics started with the review of a paper, which constructed a set of rating scales for comparing school chairs by ergonomics criteria. The authors followed the psychometric workflow neatly and collected data from a substantial sample of participants. There was only one catch: They all sat on one and same chair! When the purpose of the instrument is to rank a set of chairs, there is no way this capacity can be evaluated using only one chair.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "It’s the cube, stupid!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "DMX_1.html",
    "href": "DMX_1.html",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "",
    "text": "3 Introduction\nIn modern industrial practice, rating scales have their place as an always available and cheap method for comparing or benchmarking designs. In the decision process, the everyday value of a rating scales stands and falls with two properties: validity and reliability.\nDeveloping a valid and reliable rating scale is quite an undertaking. Psychometrics is the science of assigning numbers to persons so that they can be compared by psychological functioning. Traditionally, this served to measure skills, such as mathematical intelligence or comprehension of language. With the time, researchers also became more interested in elusive properties of persons, such as psycho-social tendencies (known as the Big Five).\nAfter the landrush phase of the UX revolution, Bargas-Avila & Hoernbaek [Old wine in new bottles] counted hundreds of new rating scale instruments. Barely any of these instrument underwent the same scrutiny as, for example, a rating scale for psycho-pathological diagnosis would have to. But, frequently some psychometric tools were used at one point during development time, for example by reporting reliability estimates. Also for users of such instruments, it is common to perform basic psychometric sanity checks on their data. The central point of this paper is a certain catch that can occur when translating between psychological research and design research, which we call the psychometric fallacy.\nIn psychometric situations, the atomic observation is an encounter of a person with a test item. This is repeated with more items to improve the precision of the measurement. If many persons are assessed this way, the result is a Person X Item response matrix, from which person scores can be extracted for ranking. Item scores, are mostly used during scale development.\nThe logical argument developed in this paper is that instruments in design research exist to rank designs and the atomic observation is an encounter of a design with a person and an item. This forms a box of data, which can be collapsed by three perspectives. We demonstrate how standard psychometric tools can be used correctly using the designometric response matrix. We call it the psychometric fallacy, when a psychometric matrix is used instead.\n…\nAim of this study is also to seek empirical evidence that the psychometric fallacy is not just sophistry, but can result in real biases when developing or using rating scales. For this purpose, data from five experiments was subjected to typical rating scale validation techniques under both perspectives, psychometric (pretending the fallacy) and designometric (using the proper response matrix).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#psychometrics",
    "href": "DMX_1.html#psychometrics",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "3.1 Psychometrics",
    "text": "3.1 Psychometrics\nIf it only takes a single probe to accurately estimate the body temperature of a person, why are psychological tests and math exams so long? Indeed, psychological properties of the persons, e.g. language skills or neuroticism, are assessed by multiple tasks or items. In a broader sense, experiments in Cognitive Psychology\nThe primarily reason is that psychological instruments are highly noisy, be it self-report scales, reaction times or physiological measures. The idea of repeated measures goes back to Classic Test Theory, which solves this problem by decomposing every measure into a relevant systematic component and a nuisance error component. By definition, the systematic component re-occurs and is strengend with every measurement, whereas the error is fully random, cancelling itself out in the long run. When certain conditions are met, it is possible to create an arbitrary precise estimate by adding more repetitions.\nThe primary practical uses of psychometric instruments are either about making predictions about a person, with their best in mind, for example when using screening tests for depression, or in competitive situations, such as personnel selection and education.\n\n\n\n\n3.1.1 Item selection\nThe first challenge when designing a psychometric inventory is to understand the domain, ideally in terms of involved areas of psychological functioning. Based on the domain analysis, the researcher creates a candidate item pool, which is usually much larger than the targeted set of items. Up to this point, the process is mostly qualitative, divergent and creative.\nSeveral psychometric methods can be used to successively reduce the initial set of items to reliable scales and to find effective multi-scale strucures.\nUnder the assumptions, the errors of multiple items cancel each other out, if they fully agree on the systematic component. This can be the case with extremely repetitive items, such as trials in the Stroop task. But in practice, item scores only agree to some degree on what they measure. Scale consistency by Cronbach \\(\\alpha\\) is a statistic to measure the overall level of agreement. A basic procedure for item selection is to compare the result of the full item set to the data set excluding item \\(i\\). If scale consistency improves the item will be kept, if it reduces, the item is marked for removal.\nThe procedure of step-wise removal is likely to fail, when the alleged property truly has more than one component. In this case, factor-analytic methods can be used to separate components into multiple factors with better consistency.\n\n\n3.1.2 Factor Structures\nIn modern psychometrics, the relation between a properties of persons and multiple measure is known as the distinction between latent variables (not observable, but true) and indicator variables (observable, but imperfect).\nModern instruments often carry a domain structure, where multiple latent variables are put in relation to each other. A prominent example is the Big 5 inventory, which claims that social behaviour can be predicted by five psycho-social traits, each of which is assessed by a separate set of items.\nWhile the primary aim of domain analysis is to gain sufficient coverage of the domain, it often at least indicates a possible structure for a multi-item instrument. In some cases, a domain analysis is driven by a theoretical structure in the first place. For example, a rating scale for mental workload could very well be based on the multiple-resource theory, which predicts that different sensory modes are processed independently, translating into one scale per sensory mode.\n\nAuditory load: It is difficult to understand the signal tones.\nVisual load: The amount of visual information can be overwhelming.\n\nPrior structures often also emerge from qualitative results in mixed-method psychometric processes. …\nWhen prior structures exist, Confirmatory Factor Analysis (CFA) is the most recommended technique to critically assess prior assumptions about the latent variable structure. For multi-scale inventories CFA also supercedes Cronbach \\(\\alpha\\), as parameter loadings can be used to identify inconsistent items.\nFor example with embedded questions in an arithmetics test, performance may depend on arithmetic processing speed, but also on text comprehension. If items in the test vary in text difficulty (and participants vary in comprehension skill), a second systematic component emerges. A common technique for finding new structures is Exploratory Factor Analysis, which requires the researcher to name the number of factors and how these factors are correlated.\nSeveral methods have been proposed to identify the optimal number of factors before-hand. Early tools used the decline in Kaiser eigenvalues and the visual elbow criterion, whereas modern tools use resampling techniques to determinje the optimal number of factors. The second choice to make is scale rotation. Orthogonal rotation applies, when the components are largely independent, such as arithmetic skills and text comprehension. For true sub scales a significant correlations between underlying factors is very likely, and oblique rotation should be used, instead.\n\n\n3.1.3 The Response Matrix\nAll psychometric methods mentioned so far have in common that measures are provided in form of response matrices, where rows identify with participants and columns with items. A common operation during item selection is to estimate consistency on the full response matrix, remove a dubious item from the matrix to see if this improves consistency. EFA takes a response matrix as input and produces item loadings for it. For multi-scale data CFA tools take response matrix with named columns as input, together with a formula to group items to factors. …\nThe idea of response matrices is most pronounced in item response theory (IRT), a psychometric approach rivaling classic test theory and factors analysis. In IRT, each cell in the response matrix is seen as an encounter between an item and a participant, from which a measure arises. Items, like participants, are seen as individuals, with different response characteristics during the encounter. In the most basic Rasch model, the probability of success in a person-task encounter depends only on the difference between task difficulty and the person ability. More complex IRT models allow more subtle item characteristics, and sophisticated tools have been developed to detect and prevent differential item functioning\nin world-spanning studies like TIMMS and PISA. IRT models take the person x item matrix as input and produce estimates on both margins, item parameters and person parameters. During the development of an IRT model, sophisticated tools are used to select only those items that strictly behave as the model states.\nFor all levels of sophistication or schools of thought, what counts during scale development is that item of a test are well-behaved. In the same way teachers use multiple tests to estimate a student’s skill with adequate accuracy, many responses are needed to precisely assess the characteristics of an item during development time. This is what creates the demand for the enormous sample sizes, for which psychometric development is most feared for.\n\n\n3.1.4 Sample sizes\nThe enormous sample sizes consumed during psychometric scale validation have several causes:\n\nPsychometric techniques tend to have many parameters and by the rule that the number of independent observations must at least match the number of parameters, a lower boundary can be established. A 24 item three-way EFA already produces 72 parameters. But, during scale validation, the number of items is often three times larger, which adds up.\nWith noisy data certain estimation procedures need a lot of data to converge properly, for example in conditional maximum likelihood estimation.\nWhen a structure is desired, but no prior structure exists, EFA can be used to find factors. But the consequence is that the data set is consumed. To confirm the found structure by CFA, a new sample is required.\nThe main areas of psychometric research are education and clinical diagnosis, where a rating scale or test can have a significant impact on a person’s life. These instruments ought to be extra-hardened against biases and require the greatest scrunity. For item in a PISA test, proof is required that it has the same characteristics all around the globe.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#designometrics",
    "href": "DMX_1.html#designometrics",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "3.2 Designometrics",
    "text": "3.2 Designometrics\nEverything said above is essentially rooted in mathematical theory, and therefore independent of semantics. Formally, constructing rating scales to rank humans should be not much different to constructing a usability metric for websites. When a UX scale is used to decide between designs A and B in industrial practice, it is legit to ask how precise the measurements are. Indeed, designometric scales can efficiently be constructed and evaluated using existing psychometric tools, such as Cronbach \\(\\alpha\\), factor analysis techniques ans IRT models for maximum rigor.\nThere are just two differences: First, in psychometrics the entity to be ranked is persons, whereas designometric applications one or more designs get numbers attached. Second, a psychometric measure is an encounter of a person with an item, whereas a designometric measure is an encounter of a design with a person and an item. The three-way encounter and designs being subject to measurement is what defines designometric situations.\n\n3.2.1 The designometric perspective\nA complete designometric data set is a cross-product of three samples, designs, users and items. It forms a “response box”, rather than a matrix, and can not directly be processed with psychometric tools.\nA practical solution is to average across one factor, which produces a flat response matrix to fit into psychometric tools. For designometric instrument validation, averaging across Person creates a design-by-item designometric response matrix. This matrix takes the place of the psychometric response matrix and researchers can use it with standard psychometric tools to select proper items and find good structures.\n\n\n3.2.2 Designometric sampling\nDuring psychometric scale development, This also implies that for a proper item selection and factorizing process a substantial sample of designs is required. This can be a huge problem, depending on the class of designs. For e-government websites a large design sample will be easier to come by than a sample of human-like robots or self-driving cars.\nWhen Design takes the place of Person, it is implied that items must now be well-behaved in ranking designs, and substantial samples of designs are now required to prove that. This can be a minor or huge problem, depending on the class of designs. For e-government websites a large design sample will be easier to come by than a sample of human-like robots or self-driving cars.\nA connected problem is that the designometric perspective participant-level information is reduced, but they are still involved in the designometric encounter. In the special case that scales are used for first impression ratings, it is possible to rund cross-product complete designometric encounters. For example, when we used the Eeriness scale on artificial faces, a single observation only took around 4 seconds, making it relatively easy to fill the designometric box with data, even repeatedly [BT Robbin Koopmans].\nWhen a real interactive experience is subject of the measure, a measurent can take from several minutes to hours and a complete designometric encounter becomes impractical. A way to mitigate this problem is to use an experimental design that is planned incomplete. Essentially, a planned incomplete validation study has all participants encounter only a partition of the design sample. For example a study consisting of a sample of 100 designs let every participant encounter a different set of ten designs. As long as all designs are covered by at least one participant, this will result in a complete design-by-item matrix after collapsing along participants.\nA variation of planned incomplete studies is to successively build the sample of designs. This is especially useful, when dealing with emerging classes of designs. This happened in the BUS-11 studies, where initially it was difficult to build a substantial sample, before it became more common.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#the-psychometric-fallacy",
    "href": "DMX_1.html#the-psychometric-fallacy",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "3.3 The psychometric fallacy",
    "text": "3.3 The psychometric fallacy\nDesignometric scales can be developed and validated with established psychometric tools, when using proper design x item response matrices and a sufficiently large sample of designs to prove that items are well behaved. In contrast, many designometric instruments have not been validated using a large sample of designs, but rather on a psychometric matrix. This we call the psychometric fallacy.\nFormally, a designometric box can produce a psychometric response matrix by averaging over Design. When a scale validation study in design research is under the psychometric fallacy, validation metrics such as item reliability may be meaningless for the purpose of ranking designs. Rather, the metric will refer to the capability of the item to discriminate persons by their sensitivity to the design feature in question. For example, a scale for comparing designs by beauty would become a scale to rank persons by how critical they are with respect to interface beauty. This is not the same and in the next section we show by simulation that the differences between designometric and psychometric perspectives can be dramatic.\nDuring the scale development process, the psychometric fallacy appears in two forms, one is repairable, whereas the other is fatal. We speak of a repairable psychometric fallacy, when the validation study properly collected designometric data, but used psychometric response matrices for validation. This can be repaired by redoing the analysis now using a designometric matrix.\nA study that did not collect a sample of designs, but used only a single or very few designs fell for the fatal psychometric fallacy. In these cases, researchers have failed to recognize a simple truth: The capacity to discriminate between designs can impossibly be validated on a single design. Every alleged designometric instrument, where this has happened during the validation phase, cannot be trusted.\nRecall that psychometric validations require large samples of participants! Similar sample sizes will be required for designs when validating designometric instruments. So, even validation studies that included multiple designs, may in practice not be repairable, because the sample of designs is too small for that type of analysis (e.g. factor analysis). To give an example [MacDorman XY] validated the Godspeed Index, a common inventory to evaluate robotic designs. While scale reliability was assessed under a psychometric perspective, validity was correctly assessed using a sample of four designs. This allowed the authors to do a rough test on the ranking capabilities of their scales, but would not be suitable assess more detailed metric properties of the scale.\nAs a milder form run-time psychometric fallacy appears when an instrument is used in practice to take measures on a single design. The result will inevitable look like a psychometric response matrix and, given that publication rules (e.g. APA guidelines) often require to report some psychometric properties, it may be tempting for the researcher to run a psychometric test on reliability. While the run-time fallacy does not have the same impact as development-time fallacies, it may have cause some head aches when a validated instrument seems to have poor reliability.\nIn the following section, we construct a case by simulation, showing that psychometric and designometric perspectives can result in dramatically different results. In the remainder of the study, we will use data from past experiments to evaluate how strong the deviations are with several existing and commonly used rating scales.\nThe formal argumeńt for the psychometric fallacy is that using psychometric response matrices would allow to construct an instrument for ranking designs using a single design.\nThe aim of the present study is to assess the possible risks and consequences of falsely using the psychometric perspective. In the following section, we construct a case by data simulation, where psychometric and designometric perspectives result are dramatically different. In the remainder of the study, we will use data from past experiments to compare psychometric versus designometric perspective on several widely used UX rating scales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#data-sets",
    "href": "DMX_1.html#data-sets",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "5.1 Data sets",
    "text": "5.1 Data sets\nThe data used for analysis originates from five experiments (DK, PS, AH, QB, DN). While these experiments were carried out to test their own hypotheses, they have in common that participants saw pictures of many designs and were asked to respond to items taken from one or more scales. In QB and DN participants saw pictures of home pages and responded to several user experience scales, whereas in AH, DK and PS the stimuli were robot faces. Some of the original experiments used manipualtion of presentation time to collect data on subconscious cognitive processing. For the analysis here, only responses at presentation times of 500ms and 2000m were used.\nPer trial participants saw a single design followed by a random single item, resulting in a sparse designometric box. However, when collapsing the box to either psychometric RM or designometric RM, the result is completely filled response matrices.\n\nload(\"DMX_data.Rda\")\n\n\nD_1 |&gt; \n  ggplot(aes(x = response)) +\n  geom_histogram() +\n  facet_wrap(~Scale, scale = \"free_y\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nD_1 |&gt; \n  distinct(Study, Design) |&gt; \n  group_by(Study) |&gt; \n  summarize(n_Design = n()) |&gt; \n  ungroup() |&gt; \n  left_join(D_1 |&gt; \n              distinct(Study, Part) |&gt; \n              group_by(Study) |&gt; \n              summarize(n_Part = n())|&gt; \n              ungroup()) |&gt; \n  left_join(D_1 |&gt; \n              group_by(Study) |&gt; \n              summarize(n_Obs = n())|&gt; \n              ungroup()\n  )\n\nJoining with `by = join_by(Study)`\nJoining with `by = join_by(Study)`\n\n\n\n\n\nStudy\nn_Design\nn_Part\nn_Obs\n\n\n\n\nAH\n20\n45\n10800\n\n\nDK\n80\n35\n2800\n\n\nDN\n48\n42\n8064\n\n\nPS\n87\n39\n2808\n\n\nQB\n76\n25\n1900\n\n\nSP\n66\n40\n1440\n\n\n\n\n\n\nD_1 |&gt; \n  distinct(Scale, Design) |&gt; \n  group_by(Scale) |&gt; \n  summarize(n_Design = n()) |&gt; \n  ungroup() |&gt; \n  left_join(D_1 |&gt; \n              distinct(Scale, Part) |&gt; \n              group_by(Scale) |&gt; \n              summarize(n_Part = n())|&gt; \n              ungroup()) |&gt; \n  left_join(D_1 |&gt; \n              group_by(Scale) |&gt; \n              summarize(n_Obs = n())|&gt; \n              ungroup()\n  )\n\nJoining with `by = join_by(Scale)`\nJoining with `by = join_by(Scale)`\n\n\n\n\n\nScale\nn_Design\nn_Part\nn_Obs\n\n\n\n\nAttractiveness\n66\n40\n1440\n\n\nBeauty\n48\n42\n2688\n\n\nCredib\n76\n25\n500\n\n\nHQI\n76\n25\n700\n\n\nHQS\n76\n25\n700\n\n\nHedonism\n48\n42\n2688\n\n\nUsability\n48\n42\n2688\n\n\nnEeriness\n127\n119\n16408\n\n\n\n\n\n\n5.1.1 Scales\nFor the following rating scales responses have been extracted from the original experimental data:\nThe Eeriness scale has been developed for measuring negative emotional responses towards robot faces and is a primary research tool on the Uncanny Valley phenomenon. Ho & MacDorman(2017) present an advanced psychometric validation of the scale. The study made use of 12 animated characters (Designs), avoiding the fatal fallacy to some degree, but the data analysis is under psychometric perspective.\nThe Attractiveness scale is part of the User Experience Questionnaire (UEQ) inventory. Is has been vaidated by [Bettina Laugwitz, Theo Held, and Martin Schrepp. 2008. Construction and Evaluation of a User Experience Questionnaire. . 63–76. https://doi.org/10.1007/978-3-540-89350-9_6] The UEQ has undergone basic psychometric evaluation in six studies with a single design each.\nThe two scales Hedonic Quality - Identity (HQI) and Hedonic Quality - Stimulation (HQS) are from the AttrakDiff2 inventory. AttrakDiff2 underwent basic evaluation using only three Designs under psychometric perspective (level 1 fallacy) [Hassenzahl, M., Burmester, M., Koller, F., AttrakDiff: Ein Fragebogen zur Messung wahrgenommener hedonischer und pragmatischer Qualität].\nThe Credibility scale … #### [HERE]\n\nD_1 |&gt; \n  group_by(Study, Scale) |&gt; \n  summarize(n_Items = n_distinct(Item),\n            n_Part = n_distinct(Part),\n            n_Design = n_distinct(Design),\n            n_Obs = n()) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'Study'. You can override using the\n`.groups` argument.\n\n\n\n\n\nStudy\nScale\nn_Items\nn_Part\nn_Design\nn_Obs\n\n\n\n\nAH\nnEeriness\n8\n45\n20\n10800\n\n\nDK\nnEeriness\n8\n35\n80\n2800\n\n\nDN\nBeauty\n4\n42\n48\n2688\n\n\nDN\nHedonism\n4\n42\n48\n2688\n\n\nDN\nUsability\n4\n42\n48\n2688\n\n\nPS\nnEeriness\n8\n39\n87\n2808\n\n\nQB\nCredib\n5\n25\n76\n500\n\n\nQB\nHQI\n7\n25\n76\n700\n\n\nQB\nHQS\n7\n25\n76\n700\n\n\nSP\nAttractiveness\n6\n40\n66\n1440\n\n\n\n\n\n\n\n5.1.2 Data analysis\nGoal of the analysis is to examine in how much the psychometric fallacy creates real biases. For this purpose, three basic psychometric techniques were applied to on several data sets:\n\nFor scale consistency the\nItem reliability\nNumber of Factors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#scale-consistency",
    "href": "DMX_1.html#scale-consistency",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "6.1 Scale consistency",
    "text": "6.1 Scale consistency\n\nScale_rel &lt;-\n  D_1 |&gt; \n  group_by(Scale) |&gt; \n  group_split() |&gt; \n  map_df(alpha_ci)\n\nSome items ( Att4 Att6 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\nScale_rel\n\n\n\n\nScale\nPerspective\ncenter\nlower\nupper\n\n\n\n\nAttractiveness\ndesignometric\n0.6392175\n0.4771069\n0.7590189\n\n\nAttractiveness\npsychometric\n0.3836383\n0.0195645\n0.6424636\n\n\nBeauty\ndesignometric\n0.9686463\n0.9545610\n0.9802048\n\n\nBeauty\npsychometric\n0.5916615\n0.4041297\n0.7331962\n\n\nCredib\ndesignometric\n0.5464101\n0.1966600\n0.7674342\n\n\nCredib\npsychometric\n0.4297211\n-0.0989566\n0.6797584\n\n\nHQI\ndesignometric\n0.6721070\n0.5133820\n0.7870292\n\n\nHQI\npsychometric\n0.6525816\n0.2717853\n0.8529657\n\n\nHQS\ndesignometric\n0.7442169\n0.6418625\n0.8222326\n\n\nHQS\npsychometric\n0.7105532\n0.3921406\n0.8803329\n\n\nHedonism\ndesignometric\n0.9686170\n0.9552764\n0.9796685\n\n\nHedonism\npsychometric\n0.6223358\n0.3949200\n0.7597665\n\n\nUsability\ndesignometric\n0.8775760\n0.8064100\n0.9232319\n\n\nUsability\npsychometric\n0.6570833\n0.4412349\n0.8307060\n\n\nnEeriness\ndesignometric\n0.8832250\n0.8534313\n0.9105487\n\n\nnEeriness\npsychometric\n0.8052611\n0.7034081\n0.8682943\n\n\n\n\n\n\nScale_rel |&gt; \n  ggplot(aes(color = Scale,\n             label = Scale,\n             x = Perspective,\n             y = center,\n             ymin = lower,\n             ymax = upper)) +\n  geom_point() +\n  geom_line(aes(group = Scale)) +\n  ylab(\"std. Cronbach alpha\") +\n  geom_label() +\n  ylim(0,1)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#item-reliability",
    "href": "DMX_1.html#item-reliability",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "6.2 Item reliability",
    "text": "6.2 Item reliability\n\nItem_rel &lt;-\n  D_1 |&gt; \n  group_by(Scale) |&gt; \n  group_split(Scale) |&gt; \n  map_df(item_rel)\n\nSome items ( Att4 Att6 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\nItem_rel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScale\nItem\nPerspective\nn\nraw.r\nstd.r\nr.cor\nr.drop\nmean\nsd\n\n\n\n\nAttractiveness\nAtt1\ndesignometric\n59\n0.6935717\n0.6312193\n0.5552990\n0.4152139\n0.5578079\n0.2357714\n\n\nAttractiveness\nAtt1\npsychometric\n40\n0.6556074\n0.6582254\n0.5850407\n0.3760495\n0.5602917\n0.1860536\n\n\nAttractiveness\nAtt2\ndesignometric\n56\n0.6803049\n0.6789821\n0.6363908\n0.5044168\n0.5051086\n0.2363659\n\n\nAttractiveness\nAtt2\npsychometric\n40\n0.6499161\n0.6604626\n0.6482586\n0.3950330\n0.4927083\n0.1721726\n\n\nAttractiveness\nAtt3\ndesignometric\n54\n0.7224083\n0.7113328\n0.6250382\n0.5291541\n0.5478719\n0.2477413\n\n\nAttractiveness\nAtt3\npsychometric\n40\n0.4999924\n0.5096504\n0.3655711\n0.1976182\n0.5500833\n0.1733970\n\n\nAttractiveness\nAtt4\ndesignometric\n57\n0.3314863\n0.3230971\n0.0892797\n0.0477289\n0.5078553\n0.2118101\n\n\nAttractiveness\nAtt4\npsychometric\n40\n0.2379467\n0.2590127\n0.0002670\n-0.0557086\n0.4937083\n0.1556704\n\n\nAttractiveness\nAtt5\ndesignometric\n53\n0.7734007\n0.7683238\n0.7427277\n0.6165928\n0.6050833\n0.2344340\n\n\nAttractiveness\nAtt5\npsychometric\n40\n0.7220290\n0.7143748\n0.6884718\n0.4761634\n0.5999583\n0.1851124\n\n\nAttractiveness\nAtt6\ndesignometric\n55\n0.5071367\n0.4889531\n0.3262131\n0.2360847\n0.5461803\n0.2407393\n\n\nAttractiveness\nAtt6\npsychometric\n40\n0.2038149\n0.1716358\n-0.1026968\n-0.1800056\n0.5563333\n0.2040698\n\n\nBeauty\n1\ndesignometric\n48\n0.9715208\n0.9705167\n0.9620374\n0.9469555\n0.4817018\n0.2405092\n\n\nBeauty\n1\npsychometric\n42\n0.5634238\n0.5646013\n0.2839047\n0.2265541\n0.4763787\n0.1232866\n\n\nBeauty\n2\ndesignometric\n48\n0.9512628\n0.9509864\n0.9304837\n0.9141279\n0.5581468\n0.2252406\n\n\nBeauty\n2\npsychometric\n42\n0.7005367\n0.7291817\n0.6326197\n0.4630836\n0.5518076\n0.1088177\n\n\nBeauty\n3\ndesignometric\n48\n0.9697443\n0.9684071\n0.9595165\n0.9437284\n0.5264599\n0.2403634\n\n\nBeauty\n3\npsychometric\n42\n0.7725085\n0.7719752\n0.7087504\n0.5186978\n0.5387326\n0.1285998\n\n\nBeauty\n4\ndesignometric\n48\n0.9328297\n0.9355976\n0.9008122\n0.8865251\n0.4910026\n0.2133545\n\n\nBeauty\n4\npsychometric\n42\n0.6487324\n0.6222997\n0.3868669\n0.3061015\n0.5009746\n0.1341732\n\n\nCredib\nCredib1\ndesignometric\n59\n0.6044867\n0.5883258\n0.4129379\n0.3185598\n0.5694068\n0.2211135\n\n\nCredib\nCredib1\npsychometric\n25\n0.6959111\n0.6899259\n0.6056964\n0.4315671\n0.5667000\n0.1462139\n\n\nCredib\nCredib2\ndesignometric\n59\n0.6551251\n0.6053937\n0.4458494\n0.3364046\n0.5215226\n0.2560754\n\n\nCredib\nCredib2\npsychometric\n25\n0.7337153\n0.7095985\n0.6784585\n0.4579035\n0.5105000\n0.1592414\n\n\nCredib\nCredib3\ndesignometric\n59\n0.6514878\n0.6416967\n0.5279867\n0.4086193\n0.5228249\n0.2228835\n\n\nCredib\nCredib3\npsychometric\n25\n0.5507187\n0.5569437\n0.3896841\n0.2571936\n0.5160000\n0.1363455\n\n\nCredib\nCredib4\ndesignometric\n57\n0.7123939\n0.7005291\n0.6093975\n0.4802624\n0.5217398\n0.2564079\n\n\nCredib\nCredib4\npsychometric\n25\n0.4871192\n0.4900538\n0.2266055\n0.1664315\n0.5203000\n0.1409701\n\n\nCredib\nCredib5\ndesignometric\n53\n0.5370583\n0.4970466\n0.2570236\n0.1843925\n0.4953751\n0.2240947\n\n\nCredib\nCredib5\npsychometric\n25\n0.3886878\n0.4118324\n0.1330502\n0.0676140\n0.4957000\n0.1353824\n\n\nHQI\nHQI1\ndesignometric\n54\n0.5650463\n0.5434197\n0.4726666\n0.3768710\n0.4896019\n0.2501074\n\n\nHQI\nHQI1\npsychometric\n25\n0.8131976\n0.7897839\n0.7924689\n0.6859397\n0.4950000\n0.2026234\n\n\nHQI\nHQI2\ndesignometric\n60\n0.6225986\n0.5833110\n0.4822048\n0.3755944\n0.5781833\n0.2724666\n\n\nHQI\nHQI2\npsychometric\n25\n0.4140515\n0.4199140\n0.2907244\n0.2108103\n0.5604000\n0.1667609\n\n\nHQI\nHQI3\ndesignometric\n58\n0.6975315\n0.6536894\n0.5659559\n0.4689579\n0.5020259\n0.2574881\n\n\nHQI\nHQI3\npsychometric\n25\n0.5941852\n0.6053038\n0.5068226\n0.4222744\n0.4941000\n0.1680958\n\n\nHQI\nHQI4\ndesignometric\n57\n0.6658530\n0.7012931\n0.6339359\n0.5353069\n0.5549269\n0.2150086\n\n\nHQI\nHQI4\npsychometric\n25\n0.5258622\n0.5417442\n0.4966752\n0.3680606\n0.5709000\n0.1448704\n\n\nHQI\nHQI5\ndesignometric\n59\n0.4338197\n0.3642983\n0.2027174\n0.1251196\n0.5912429\n0.2374892\n\n\nHQI\nHQI5\npsychometric\n25\n0.7795347\n0.7634402\n0.7937611\n0.6220707\n0.5843000\n0.2158794\n\n\nHQI\nHQI6\ndesignometric\n63\n0.5748955\n0.5799387\n0.4509783\n0.3832471\n0.5124471\n0.2509687\n\n\nHQI\nHQI6\npsychometric\n25\n0.5138991\n0.5250801\n0.3944244\n0.3154123\n0.5037000\n0.1751995\n\n\nHQI\nHQI7\ndesignometric\n57\n0.7367313\n0.7135030\n0.6752447\n0.6019045\n0.4723392\n0.2803627\n\n\nHQI\nHQI7\npsychometric\n25\n0.5576736\n0.5584739\n0.4336605\n0.3685304\n0.4573000\n0.1750787\n\n\nHQS\nHQS1\ndesignometric\n53\n0.6810616\n0.6438297\n0.5481391\n0.5024558\n0.5541855\n0.2449457\n\n\nHQS\nHQS1\npsychometric\n25\n0.6168159\n0.6070060\n0.5477540\n0.4332504\n0.5197000\n0.1599859\n\n\nHQS\nHQS2\ndesignometric\n55\n0.7521635\n0.7645039\n0.7239777\n0.6402145\n0.3685152\n0.2426397\n\n\nHQS\nHQS2\npsychometric\n25\n0.6922924\n0.7099388\n0.6440410\n0.5659545\n0.3813000\n0.1321243\n\n\nHQS\nHQS3\ndesignometric\n56\n0.5881881\n0.5813283\n0.5070851\n0.4181500\n0.4187946\n0.2243515\n\n\nHQS\nHQS3\npsychometric\n25\n0.7746096\n0.7598930\n0.7075289\n0.6311833\n0.4277000\n0.1744156\n\n\nHQS\nHQS4\ndesignometric\n61\n0.6705848\n0.6303619\n0.5669467\n0.4614471\n0.4811749\n0.2632831\n\n\nHQS\nHQS4\npsychometric\n25\n0.4887681\n0.4886682\n0.3808049\n0.2910595\n0.4654000\n0.1498588\n\n\nHQS\nHQS5\ndesignometric\n56\n0.6387364\n0.5288620\n0.4422436\n0.3392942\n0.3966518\n0.2557008\n\n\nHQS\nHQS5\npsychometric\n25\n0.5210489\n0.5184179\n0.4270064\n0.3229098\n0.3899000\n0.1542091\n\n\nHQS\nHQS6\ndesignometric\n61\n0.7125647\n0.6658936\n0.5856063\n0.4876641\n0.4197322\n0.2625926\n\n\nHQS\nHQS6\npsychometric\n25\n0.5634879\n0.5772406\n0.4567243\n0.3922267\n0.4003000\n0.1417273\n\n\nHQS\nHQS7\ndesignometric\n61\n0.5840752\n0.5786060\n0.4860787\n0.3960248\n0.4128005\n0.2460413\n\n\nHQS\nHQS7\npsychometric\n25\n0.6966990\n0.6955517\n0.6761135\n0.5369481\n0.4267000\n0.1611009\n\n\nHedonism\n1\ndesignometric\n48\n0.9754944\n0.9729531\n0.9661373\n0.9489504\n0.4949129\n0.2486562\n\n\nHedonism\n1\npsychometric\n42\n0.5736808\n0.6387250\n0.4451207\n0.3110343\n0.4989629\n0.0873503\n\n\nHedonism\n2\ndesignometric\n48\n0.9265228\n0.9346126\n0.9044443\n0.8870970\n0.5479773\n0.1613367\n\n\nHedonism\n2\npsychometric\n42\n0.6375588\n0.6812583\n0.5040558\n0.3795473\n0.5420985\n0.0918896\n\n\nHedonism\n3\ndesignometric\n48\n0.9539940\n0.9513429\n0.9322637\n0.9167024\n0.4675383\n0.2110448\n\n\nHedonism\n3\npsychometric\n42\n0.7611141\n0.7352989\n0.6478569\n0.5326241\n0.4721382\n0.1008703\n\n\nHedonism\n4\ndesignometric\n48\n0.9644170\n0.9626921\n0.9502177\n0.9360589\n0.4408734\n0.2070048\n\n\nHedonism\n4\npsychometric\n42\n0.7802483\n0.7127237\n0.6195312\n0.4669670\n0.4360133\n0.1286006\n\n\nUsability\n1\ndesignometric\n48\n0.8954695\n0.8852177\n0.8496008\n0.8072696\n0.5537115\n0.1698535\n\n\nUsability\n1\npsychometric\n42\n0.7059783\n0.6963695\n0.5421684\n0.4300962\n0.5353763\n0.1208224\n\n\nUsability\n2\ndesignometric\n48\n0.8949999\n0.8727338\n0.8517108\n0.8006296\n0.5615960\n0.1771572\n\n\nUsability\n2\npsychometric\n42\n0.6888980\n0.7014512\n0.5453210\n0.4359088\n0.5574644\n0.1107410\n\n\nUsability\n3\ndesignometric\n48\n0.6282174\n0.6911338\n0.5121547\n0.4845593\n0.5668973\n0.1168843\n\n\nUsability\n3\npsychometric\n42\n0.6876127\n0.6760119\n0.4882153\n0.4002469\n0.5600007\n0.1214538\n\n\nUsability\n4\ndesignometric\n48\n0.9622228\n0.9443958\n0.9610464\n0.9097243\n0.5221123\n0.2167186\n\n\nUsability\n4\npsychometric\n42\n0.7434466\n0.7530564\n0.6456878\n0.5171022\n0.5242849\n0.1112598\n\n\nnEeriness\nnE1\ndesignometric\n127\n0.7408118\n0.7337785\n0.6851655\n0.6440660\n0.4678868\n0.1740001\n\n\nnEeriness\nnE1\npsychometric\n119\n0.5160214\n0.5343692\n0.4265733\n0.3863068\n0.4556709\n0.0952187\n\n\nnEeriness\nnE2\ndesignometric\n126\n0.7428029\n0.7396144\n0.6922272\n0.6406685\n0.4809049\n0.1821338\n\n\nnEeriness\nnE2\npsychometric\n119\n0.6873760\n0.6728223\n0.6097037\n0.5412618\n0.4710938\n0.1331084\n\n\nnEeriness\nnE3\ndesignometric\n125\n0.7363558\n0.7059386\n0.6480533\n0.5971234\n0.4606730\n0.2197593\n\n\nnEeriness\nnE3\npsychometric\n119\n0.5663081\n0.5477541\n0.4419186\n0.3869046\n0.4442411\n0.1334954\n\n\nnEeriness\nnE4\ndesignometric\n126\n0.6349151\n0.6613785\n0.5899471\n0.5480713\n0.5499998\n0.1425516\n\n\nnEeriness\nnE4\npsychometric\n119\n0.6343404\n0.6317383\n0.5547755\n0.4991363\n0.5278561\n0.1144561\n\n\nnEeriness\nnE5\ndesignometric\n127\n0.7501838\n0.7596466\n0.7176147\n0.6736243\n0.5490203\n0.1549981\n\n\nnEeriness\nnE5\npsychometric\n119\n0.7423577\n0.7477601\n0.7092518\n0.6486944\n0.5340442\n0.1036929\n\n\nnEeriness\nnE6\ndesignometric\n127\n0.7180501\n0.7177474\n0.6620540\n0.6239836\n0.5236777\n0.1610552\n\n\nnEeriness\nnE6\npsychometric\n119\n0.6432526\n0.6575557\n0.5929158\n0.5329360\n0.5006405\n0.0974882\n\n\nnEeriness\nnE7\ndesignometric\n125\n0.8272383\n0.8250614\n0.8087628\n0.7584556\n0.4967029\n0.1799651\n\n\nnEeriness\nnE7\npsychometric\n119\n0.7977637\n0.7956693\n0.7821557\n0.7006352\n0.4942902\n0.1245290\n\n\nnEeriness\nnE8\ndesignometric\n126\n0.7624603\n0.7763000\n0.7509644\n0.6787708\n0.5527972\n0.1646881\n\n\nnEeriness\nnE8\npsychometric\n119\n0.7030982\n0.7086295\n0.6631192\n0.5909101\n0.5344227\n0.1103392\n\n\n\n\n\n\nItem_rel |&gt; \n  ggplot(aes(x = Perspective,\n             y = r.cor)) +\n  geom_line(aes(group = Item)) +\n  ylab(\"Item-whole correlation\") +\n  geom_label(aes( label = Item)) +\n  facet_wrap(~Scale, ncol = 2) +\n  geom_point(data = rename(Scale_rel, alpha = center),\n             aes(x = Perspective, \n                 y = alpha,\n                 col = \"Whole Cronbach alpha\")) +\n  geom_line(data = rename(Scale_rel, alpha = center),\n             aes(x = Perspective, \n                 y = alpha,\n                 group = Scale,\n                 col = \"Whole Cronbach alpha\"))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#number-of-factors",
    "href": "DMX_1.html#number-of-factors",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "6.3 Number of factors",
    "text": "6.3 Number of factors\nOften, different scales are used in combination to create a more complete picture. It is usually the aim that a scale measures exactly one construct (or latent variable) and that different scales measure different constructs. As a counter-example, MacDorman found that the Eeriness scale decomposes into two slightly different aspects, summarized as “eery” and “spine-tingling”. In contrast, the AttrakDiff2 questionnaire comprises two scales to capture supposedly different aspects.\nGiven a response matrix, the number of factors can be estimated using parallel analysis. Ideally, this procedure returns exactly as many factors as there are separate scales. Here, we use parallel analysis to assess whether the two perspectives produce the expected number of factors.\n\nparallel_analysis &lt;- function(data, n, persp, scales){\n  if (persp == \"D\") {\n    data &lt;- rm_design(data)\n    main &lt;- str_c(\"Designometric Parallel Analysis of \", scales)\n  }\n  if (persp == \"P\") {\n    data &lt;- rm_psycho(data)\n    main &lt;- str_c(\"Psychometric Parallel Analysis of \", scales)\n  }\n  psych::fa.parallel(data,\n                   fa = \"fa\",\n                   fm = \"minres\",\n                   nfactors=n,\n                   main=main)\n    \n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#eeriness-scale",
    "href": "DMX_1.html#eeriness-scale",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "6.4 Eeriness scale",
    "text": "6.4 Eeriness scale\nEeriness is usually considered a one-dimensional construct. Nevertheless, it has been suggested that it comprises two slightly different factors.\n\nparallel_analysis(D_Eer, 2, \"D\", \"Eeriness\")\n\n`summarise()` has grouped output by 'Design'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  1  and the number of components =  NA \n\nparallel_analysis(D_Eer, 2, \"P\", \"Eeriness\")\n\n`summarise()` has grouped output by 'Part'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n\nThe results suggest that under psychometric perspective there is only one latent variables, whereas whereas the designometric perspective produces two, indeed.\n\nefa &lt;- function(rm, nfactors) psych::fa(rm_design(D_Eer),\n                        fa = \"fa\", fm = \"minres\",\n                        main=main)\n\nEfa_psycho &lt;- efa(rm_psycho(D_Eer), nfactors=1)\n\n`summarise()` has grouped output by 'Design'. You can override using the\n`.groups` argument.\n\nEfa_design &lt;- efa(rm_design(D_Eer), nfactors=2)\n\n`summarise()` has grouped output by 'Design'. You can override using the\n`.groups` argument.\n\npsych::fa(rm_psycho(D_Eer))\n\n`summarise()` has grouped output by 'Part'. You can override using the\n`.groups` argument.\n\n\nFactor Analysis using method =  minres\nCall: psych::fa(r = rm_psycho(D_Eer))\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR1   h2   u2 com\nnE1 0.42 0.17 0.83   1\nnE2 0.61 0.37 0.63   1\nnE3 0.43 0.19 0.81   1\nnE4 0.56 0.31 0.69   1\nnE5 0.71 0.51 0.49   1\nnE6 0.60 0.36 0.64   1\nnE7 0.79 0.63 0.37   1\nnE8 0.68 0.46 0.54   1\n\n                MR1\nSS loadings    3.00\nProportion Var 0.38\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\ndf null model =  28  with the objective function =  2.32 with Chi Square =  265.57\ndf of  the model are 20  and the objective function was  0.22 \n\nThe root mean square of the residuals (RMSR) is  0.05 \nThe df corrected root mean square of the residuals is  0.06 \n\nThe harmonic n.obs is  119 with the empirical chi square  19.86  with prob &lt;  0.47 \nThe total n.obs was  119  with Likelihood Chi Square =  24.55  with prob &lt;  0.22 \n\nTucker Lewis Index of factoring reliability =  0.973\nRMSEA index =  0.043  and the 90 % confidence intervals are  0 0.095\nBIC =  -71.03\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   MR1\nCorrelation of (regression) scores with factors   0.92\nMultiple R square of scores with factors          0.85\nMinimum correlation of possible factor scores     0.70\n\nE_psycho &lt;- tibble(Perspective = \"psychometric\",\n                   Item = str_c(\"nE\", 1:8),\n                   loading = psych::fa(rm_psycho(D_Eer))$loadings)\n\n`summarise()` has grouped output by 'Part'. You can override using the\n`.groups` argument.\n\nE_design &lt;- tibble(Perspective = \"designometric\",\n                   Item = str_c(\"nE\", 1:8),\n                   loading = Efa_design$loadings)\n\nbind_rows(E_psycho, E_design) |&gt; \n  ggplot(aes(x = Perspective, col = Item, y = loading)) +\n  geom_point() +\n  geom_line(aes(group = Item))\n\nNew names:\nDon't know how to automatically pick scale for object of type &lt;loadings&gt;.\nDefaulting to continuous.\n• `nE1` -&gt; `nE1...1`\n• `nE2` -&gt; `nE2...2`\n• `nE3` -&gt; `nE3...3`\n• `nE4` -&gt; `nE4...4`\n• `nE5` -&gt; `nE5...5`\n• `nE6` -&gt; `nE6...6`\n• `nE7` -&gt; `nE7...7`\n• `nE8` -&gt; `nE8...8`\n• `nE1` -&gt; `nE1...9`\n• `nE2` -&gt; `nE2...10`\n• `nE3` -&gt; `nE3...11`\n• `nE4` -&gt; `nE4...12`\n• `nE5` -&gt; `nE5...13`\n• `nE6` -&gt; `nE6...14`\n• `nE7` -&gt; `nE7...15`\n• `nE8` -&gt; `nE8...16`\n\n\n\n\n\n\n\n\nknitr::kable(bind_rows(E_psycho, E_design))\n\nNew names:\n• `nE1` -&gt; `nE1...1`\n• `nE2` -&gt; `nE2...2`\n• `nE3` -&gt; `nE3...3`\n• `nE4` -&gt; `nE4...4`\n• `nE5` -&gt; `nE5...5`\n• `nE6` -&gt; `nE6...6`\n• `nE7` -&gt; `nE7...7`\n• `nE8` -&gt; `nE8...8`\n• `nE1` -&gt; `nE1...9`\n• `nE2` -&gt; `nE2...10`\n• `nE3` -&gt; `nE3...11`\n• `nE4` -&gt; `nE4...12`\n• `nE5` -&gt; `nE5...13`\n• `nE6` -&gt; `nE6...14`\n• `nE7` -&gt; `nE7...15`\n• `nE8` -&gt; `nE8...16`\n\n\n\n\n\nPerspective\nItem\nloading\n\n\n\n\npsychometric\nnE1\n0.4160874\n\n\npsychometric\nnE2\n0.6123569\n\n\npsychometric\nnE3\n0.4303772\n\n\npsychometric\nnE4\n0.5607379\n\n\npsychometric\nnE5\n0.7124984\n\n\npsychometric\nnE6\n0.6003297\n\n\npsychometric\nnE7\n0.7940718\n\n\npsychometric\nnE8\n0.6759651\n\n\ndesignometric\nnE1\n0.6881137\n\n\ndesignometric\nnE2\n0.6932071\n\n\ndesignometric\nnE3\n0.6431185\n\n\ndesignometric\nnE4\n0.5832405\n\n\ndesignometric\nnE5\n0.7256299\n\n\ndesignometric\nnE6\n0.6587066\n\n\ndesignometric\nnE7\n0.8221862\n\n\ndesignometric\nnE8\n0.7478483",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#attrakdiff-and-credibility",
    "href": "DMX_1.html#attrakdiff-and-credibility",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "6.5 AttrakDiff and Credibility",
    "text": "6.5 AttrakDiff and Credibility\nOn theoretical grounds, the AttrakDiff2 inventory splits hedonistic quality into two components, Identity and Stimulation, while the credibility scale is a complete separate construct . Depending on whether Hedonism splits up like expected by theory, we would expect a two or three-factor solution.\n\nparallel_analysis(D_Att, 3, \"P\", \"AttrakDiff and Credibility\")\n\n`summarise()` has grouped output by 'Part'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  1  and the number of components =  NA \n\nparallel_analysis(D_Att, 3, \"D\", \"AttrakDiff and Credibility\")\n\n`summarise()` has grouped output by 'Design'. You can override using the\n`.groups` argument.\n\n\nWarning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\nThe estimated weights for the factor scores are probably incorrect.  Try a\ndifferent factor score estimation method.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n\nUnder a psychometric perspective, all items represent a single latent construct. In contrast, the designometric analysis yielded five factors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#hedonism-usability-and-beauty",
    "href": "DMX_1.html#hedonism-usability-and-beauty",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "6.6 Hedonism, Usability and Beauty",
    "text": "6.6 Hedonism, Usability and Beauty\nIn DN three separate scales were used, but parallel analysis suggests that these capture the same latent variable under both perspectives.\n\nparallel_analysis(D_HUB, 3, \"P\", \"Hedonism, Usability and Beauty\")\n\n`summarise()` has grouped output by 'Part'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  1  and the number of components =  NA \n\nparallel_analysis(D_HUB, 3, \"D\", \"Hedonism, Usability and Beauty\")\n\n`summarise()` has grouped output by 'Design'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  1  and the number of components =  NA",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#run-time-implications",
    "href": "DMX_1.html#run-time-implications",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.2 Run time implications",
    "text": "7.2 Run time implications\nWhen looking at real data from commonly used rating scales, a clear bias appears, but this time the Designometric perspective significantly improves reliability. This is good news for future users of these scales, as they can reduce their sample sizes.\nPast users may have scratched their heads more often than we know. Techniques such as Cronbach \\(\\alpha\\) are commonly applied as quick sanity checks, whenever multi-item scales are used in run-time studies. Everyone in the past who used designometric scales and followed APA guidelines, was likely to the psychometric fallacy and experience some unnecessary headaches.\nWith better item-level reliability, experiments can also be streamlined by using a reduced set of items. Reducing a scale is basically item selection and several strategies are possible, as long as it is carried out on a design-by-item matrix.\nA basic strategy is to use item-level reliability a ranking criterion. In many cases this may work well, but takes the risk of inadvertently reducing the precision in certain ranges of the scale. A well-constructed scale produces similarly precise measures in the lower, middle and high range on the whole scale. During development time this can be achieved by selecting items that are varying in strength, like “The interface is beautiful.” is stronger than “… pleasant”. By taking the average score",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#implications-for-scale-development",
    "href": "DMX_1.html#implications-for-scale-development",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.2 Implications for scale development",
    "text": "7.2 Implications for scale development\nThe UX revolution in design research brought with iIn the present, two emerging domains of human-technology interaction are gaining significant momentum:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#limitations",
    "href": "DMX_1.html#limitations",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.3 Limitations",
    "text": "7.3 Limitations\n\nPopulations in the samples were rather homogenous (students). Too little variance in the sample?\njust stimuli, no use. we can assume dominance of system 1.\nTested conditions were on finalized scales, rather than initial item pools.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#the-ideal-designometric-scale",
    "href": "DMX_1.html#the-ideal-designometric-scale",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.4 The ideal designometric scale",
    "text": "7.4 The ideal designometric scale\nThe comparison of item-level reliability suggests that the scales fall into two clusters: beauty and hedonism have overall excellent item and scale reliability. Reliability under psychometric perspective is still good. What is striking is that item reliability seem to drop by a constant",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#dev-time-implications",
    "href": "DMX_1.html#dev-time-implications",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.1 Dev time implications",
    "text": "7.1 Dev time implications",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#criticism-of-individual-scales",
    "href": "DMX_1.html#criticism-of-individual-scales",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.3 Criticism of individual scales",
    "text": "7.3 Criticism of individual scales\nOne can rightfully argue that we used the rating scales outside their specification, as the encounters were brief and without interaction. This depends on the scale and on the purpose. Face processing is one of the fastest complex mechanisms in the human mind, which makes it legit to test it in quick succession on a screen. Similarly, the beauty judgement of a website is known to stabilize within 500ms, and the same can be expected for attractiveness. For more longitudinal feelings about designs, like hedonism, usability and credibility, the encounters were not valid. We will therefore not criticize these instruments, the point still standing that the two perspectives produce different results.\n\n7.3.1 Eeriness\n\n\n7.3.2 Beauty\n\n\n7.3.3 Attractiveness",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#future-applications",
    "href": "DMX_1.html#future-applications",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.4 Future Applications",
    "text": "7.4 Future Applications\nA founding idea in usability engineering is that a good designers has learned to bridge the gap between the system model and the users mental model, cognitive skills and feelings. Emerging domains are often characterized by an innovation phase, where multiple design paths are explored, before this diversity converges into an oligarchy of mature sibling frameworks with established best practices and quality principles.\nTwo emerging domains of human-technology interaction are currently gaining significant momentum: interactive deep learning models, such as chatbots and social robots. Both fields have in common that they tap much stronger into the social mind of users, which, next to being a cave of snakes, is mostly uncharted terrain. We can expect a wild growth of theoretical concepts and instruments to measure the respective latent variables.\n[Simone]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  },
  {
    "objectID": "DMX_1.html#towards-deep-designometrics",
    "href": "DMX_1.html#towards-deep-designometrics",
    "title": "2  Designometric models for the evaluation of designs.",
    "section": "7.5 Towards Deep Designometrics",
    "text": "7.5 Towards Deep Designometrics\nBy comparing the two perspectives, we illustrated that designometric analysis can fully be done with standard psychometric tools, as long as one uses the correct response matrix. However, by reducing the designometric box to a flat matrix, we loose all information on users. Formally, it would even be possible to evaluate a designometric model on the responses of a single user, while the situation\nIf the cube is collapsed to a psychometric matrix, which can be used to estimate user sensitivity. Legitimate cases exist to use a designometric scale for psychometric purposes. For example, an instrument to measure trustworthiness of designs could be used to estimate faithfulness levels of participants in a study (or a training) on cyber security.\nBy flattening the designometric box one way, then the other, we still loose information that is needed to secure that items are truly well-behaved. In educational psychometrics differential item functioning is the idea that items must be fair and function the same for every tested person. This also is a desirable property for a designometric scale, but a statistical model for verification would need individual parameters for participants, designs and items, simultaneously.\nSchmettow(2021) proposed multi-level models for capturing designometric situations in their full dimension, which copuld be well-suited for run-time use or basic scale development. But, a full development workflow is lacking.\nAnother consideration is that the designometric encounter may not be end of story. For example, for comparing multi-purpose designs a researcher may want to add tasks as fourth population of interest. With the mentioned limitations, multi-level models extend to such a case (Schmettow, 2016, Egan’s assumption). For development-time purposes, Generalizability Theory may provide …\n[Stephanie]\nAnother issue is to identify exploratory methods that can operate on designometric boxes. EFA is often used with CFA to find and then confirm candidate structures.\n[Stephanie]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Designometric models for the evaluation of designs.</span>"
    ]
  }
]