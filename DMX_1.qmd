---
title: "The designometric perspective and the psychometric fallacies"
author: "Martin Schmettow, Simone Borsci, Stephanie van den Berg"
format: docx
editor: visual
bibliography: dmx.bib
csl: apa.csl
execute:
  echo: false
  warning: false
  message: false
  cache: false
---

# Introduction

Rating scales constitute a fundamental measurement instrument in contemporary industrial applications, providing cost-effective and readily accessible methods for comparative evaluation and benchmarking of design artifacts. The utility of rating scales in decision-making processes is contingent upon two critical psychometric properties: validity and reliability.

The development of effective rating scales is a methodologically rigorous endeavor. Psychometrics, the scientific discipline concerned with the quantitative assessment of psychological attributes to enable comparative evaluation of individual performance and functioning, has traditionally focused on cognitive abilities such as mathematical reasoning and linguistic comprehension. Subsequently, the field expanded to encompass measurement of latent psychological constructs, including personality dimensions (e.g., the Five-Factor Model).

Following the rapid expansion of user experience (UX) research, @Bargas-Avila2011 documented the proliferation of hundreds of novel rating scale instruments. However, the majority of these instruments have not undergone the rigorous psychometric validation procedures typically required for clinical assessment tools. Nevertheless, psychometric methodologies are occasionally employed during instrument development phases, particularly for reliability estimation and subscale identification through factor analysis. Additionally, practitioners implementing designometric instruments may conduct preliminary evaluations to ensure data quality and integrity.

The central theoretical argument in this investigation posits that design research instruments, specifically rating scales, function primarily to establish rank orderings among design alternatives. Consequently, the development of designometric instruments necessitates evaluation across extensive design samples to assess their ranking capabilities. This requirement results in a three-dimensional data structure (design × person × item), which is an extension of the two-dimensional response matrices (person × item) used in traditional psychometric analyses.

We introduce the phenomenon of the "psychometric fallacy," which occurs when designometric rating scales are evaluated using conventional psychometric response matrices (person × item). This approach contradicts the fundamental requirement that design samples must be incorporated into the validation process. Numerous purported designometric instruments have been developed under this fallacy, potentially compromising their capacity to effectively discriminate among design alternatives.

The practical contribution of this work demonstrates that psychometric analytical tools remain applicable to designometric data through dimensional reduction of the three-dimensional array into a design × item response matrix. This transformation constitutes solely a semantic reinterpretation, whereby statistical measures originally developed for ranking individuals are reapplied to ranking design artifacts.

Given that psychometric analytical tools typically require two-dimensional input matrices, and designometric data cubes can be collapsed along either dimension to yield either designometric (design × item) or psychometric (person × item) matrices, the implications of the psychometric fallacy can be systematically investigated through comparative analyses using both matrix configurations.

This investigation establishes the designometric measurement perspective and demonstrates the appropriate application of fundamental psychometric tools for both the development and practical implementation of designometric scales. The consequences of the psychometric fallacy are further examined through secondary analysis of designometric data collected using eight widely employed UX rating scales.

## Principles of Psychometric Test Development

Psychometric instruments serve primarily as cost-effective predictive tools for human performance and psychological attributes. Psychodiagnostic rating scales function as screening instruments for conditions such as depression [@Kroenke2001], while performance assessments facilitate decision-making in personnel selection and educational contexts [@Schmidt1998].

Three psychometric schools of thought provide the theoretical framework under which multi-item instruments yield reliable individual rankings: Classical Test Theory (CTT) focusses mainly on the aspect of error reduction through repetition, whereas Item Response Theory (IRT) adds more rigor to item selection and Factor Analysis (FA) provides support for multi-dimensional constructs.

### Item Selection and Scale Development

Psychological assessments necessitate multiple items due to inherent measurement error across self-report scales, reaction time measures, and physiological indicators. The central theoretical insight of CTT is that taking repeated measures improves measurement precision through error reduction, as formalized by the Law of Large Numbers. In practice, this is achieved by using sets of items.

Multi-item scale construction begins with comprehensive domain analysis to identify relevant psychological processes and behavioral dimensions. Following domain specification, researchers develop extensive item pools that typically exceed the target scale length. This initial phase employs qualitative, divergent methodologies emphasizing content validity and theoretical coverage [@Hinkin1998].

Subsequent item selection procedures employ quantitative criteria including item-total correlations, factor loadings, and reliability coefficients. The iterative selection process aims to optimize measurement accuracy while maintaining domain representativeness. According to CTT principles, measurement errors across multiple items cancel--provided systematic variance components demonstrate strong intercorrelation [@Cronbach1951].

Cronbach's alpha (α) quantifies internal consistency by measuring inter-item agreement within the response matrix [@Cronbach1951]. Item selection procedures compare full-scale reliability against reliability estimates with individual items removed. Items whose removal improves overall reliability are identified for elimination. Similar procedures examine item-total correlations to identify poorly performing indicators [@Clark1995].

### Factor Structures and Latent Variable Models

Stepwise item removal procedures prove adequate for unidimensional constructs but may produce unstable results when multiple components comprise the measured domain. Factor-analytic methods therefore serve to identify and separate distinct components into psychometrically sound subscales [@Fabrigar1999].

Contemporary psychometric theory distinguishes between latent variables (unobservable true scores) and indicator variables (observable but imperfect measurements). Complex instruments often incorporate multiple latent variables representing distinct domain aspects. As an examnple, the Five-Factor Model proposes that personality can be assessed through five primary traits, each measured via multi-item subscales [@McAdams1992]. While primary factors demonstrate relative independence by design, subscales within factors exhibit stronger intercorrelations.

Domain analysis often suggests potential factor structures, particularly when theoretical frameworks guide instrument development. For example, mental workload assessment scales may derive from Multiple Resource Theory, which predicts independent processing of sensory modalities, thereby supporting separate subscales for each sensory channel [@Wickens2002].

When theoretical structures exist a priori, Confirmatory Factor Analysis (CFA) provides the optimal approach for testing structural assumptions about latent variable relationships [@Brown2015]. Hierarchical CFA models can verify independence among primary scales while confirming stronger correlations among subscales.

Exploratory Factor Analysis (EFA) serves to identify novel factor structures when robust theoretical frameworks are unavailable (@Costello2005). EFA requires researchers to specify the number of factors and their correlation structure. This is inconvenient because an unknown factor structure implies unknown number of factors.

Finding the number of factors is possible by successively increasing the number of factors as long as a factor retention criterion is met. Common criteria include the Kaiser-Guttman rule (eigenvalues \> 1) and scree plot inspection, but these are have been criticized for being to lenient and subjective [@Hayton2004]. A more accurate alternative is parallel analysis, which compares observed eigenvalues to those obtained from data of identical size but randomized through resampling [@Lim2019].

Another decision to make is factor rotation which depends on theoretical expectations: orthogonal rotation applies when components are independent (e.g., mathematical and verbal abilities), while oblique rotation accommodates correlated factors typical of subscales [@Fabrigar1999].

### Response Matrix Structure and Item Response Theory

Traditional psychometric methods operate on response matrices with participants as rows and items as columns. Standard procedures include computing reliability estimates on complete matrices, evaluating consistency improvements following item removal, and conducting factor analyses on participant × item data structures.

Item Response Theory (IRT) represents an alternative framework that treats response matrices as collections of person-item encounters [@Embretson2013]. Unlike CTT, IRT models both person and item parameters simultaneously, allowing formal specification and empirical testing of item characteristics. The Rasch model, representing the simplest case of unidimensional and dichotomous responses, specifies that response probability depends solely on the difference between person ability and item difficulty [@Rasch1960].\
Advanced IRT applications include differential item functioning detection to identify and prevent measurement bias across demographic groups [@Penfield2000].

### Sample Size Requirements in Psychometric Development

Irrespective of theoretical orientation or analytic sophistication, all psychometric instrument development activities center on establishing psychometrically sound item sets. The substantial participant samples required throughout this developmental process represent one of the most challenging aspects of psychometric research. These requirements stem from several converging methodological imperatives.

Sample size must at least match the number of free parameters in the analytic model to ensure identifyability, but in practice this is usually not suficient [@Bollen1990]. In standard Confirmatory Factor Analysis applications, each item contributes two free parameters (intercept and factor loading), requiring participant-to-parameter ratios of 5:1 to 20:1 [@Brown2015]. Contemporary simulation studies suggest 200-500 participants typically suffice for well-specified models with strong factor loadings, while complex structures may require 1000 or more participants [@Wolf2013].

When theoretical structures are absent, Exploratory Factor Analysis can be used to identify suitable subscales. However, these procedures are sample-dependent, with solutions potentially capitalizing on chance or researchers' degrees of freedom [@Simmons2011]. Cross-validation procedures therefore necessitate data splitting: EFA on one subsample followed by CFA on another, effectively doubling sample requirements [@Anderson1988].

## Designometrics

Psychometrics as a formal theory describes how a set of differing instruments can produce a combined metric on which to compare the measured entities. Formally, it should not matter much to construct a web usability rating scale or a human skill test. Yet, there is an important difference that defines the *designometric situation*: First, psychometric measures form a two-way encounter, whereas comparative design studies has Design as an additional entity, forming a *response box*. Second, in psychometrics the entity to be ranked is Person, whereas designometric applications designs are compared. Whatever role Person parameters play in psychometric processes, must now be assigned to the Design parameters.

### The designometric perspective

A practical implication of the designometric perspective is that psychometric tools take flat response matrices as input and are unfit to process higher-dimensional data. While "deep" designometric models can be constructed using multi-level models [@Schmettow2021, 307-323], a practical solution exists to put psychometric tools to use. By averaging over Person, a two-dimensional response matrix can be constructed from a designometric box. This produces a *designometric response matrix* (design x item), which in turn is needed to assess the item properties with respect to ranking designs.

For a reliable designometric scale, its items must inter-correlate strongly, which can only happen when referred-to design features coincide. Take as an example a hypothetical scale to measure trustworthiness of robot faces, with two sub-scales, Realism and Likability. The impression of realism can be triggered by different features of the face, such as skull shape, details in the eyes region and skin texture. For a proper scale on realism, it would be required that these features correlate, and this essentially is a property of the robot face design process. It is a quite strong assumption that the effort a robot face designers puts into the eyes region must be strongly correlated with the effort put into skin texture, but by using psychometric models with designometric data, assumptions like these can be tested.

### Designometric scale development

Analog to psychometric scale evaluation, substantial samples of designs are required for item selection and factor identification. This can be a huge problem, depending on the class of designs. For e-government websites it will be easier compared to a scale dedicated to human-like robots or self-driving cars.

When a real interactive experience is subject of the measure, a measurement can take from several minutes to hours and a complete experimental design with every possible encounter becomes impractical. A way to mitigate this problem is to use *planned incomplete* experimental designs. Essentially, a planned incomplete validation study has all participants encounter only a partition of the design sample. For example, a sample of 100 designs can be tested by letting every participant encounter overlapping subsets. As long as all designs are covered by at least one participant, this will result in a complete design-by-item matrix after collapsing along participants.

A variation of planned incomplete studies is to *successively* build the sample of designs. This is especially useful, when dealing with emerging classes of designs. This happened in the BUS-11 studies, where initially it was difficult to build a substantial sample, before large language models broke through [@Borsci2024].

### The psychometric fallacy

Designometric scales can be developed with psychometric tools, if using design x item matrices with sufficiently large sample of designs. In contrast, many designometric instruments have not been validated using a large sample of designs, but rather on a psychometric matrix. This we call the *psychometric fallacy*.

A purportedly designometric instrument that has been validated on a single or very few designs fell for the *fatal psychometric fallacy*. In these cases, researchers have failed to recognize a simple truth: The capacity to discriminate between designs can impossibly be validated on a single design. Every alleged designometric instrument, where this has happened during the validation phase, cannot be trusted.

If a substantial sample of designs has been collected, a correct designometric response matrix can be created by averaging over Persons. However, the standard terminology in psychometric tools may still mislead the researcher to believe that producing a psychometric matrix is correct.

When a scale validation study in design research is under the psychometric fallacy, validation metrics such as item reliability may be meaningless for the purpose of ranking designs. Rather, the metric will refer to the capability of the item to discriminate persons by their sensitivity to the design feature in question. For example, a scale for comparing designs by beauty would become a scale to rank persons by how critical they are with respect to interface aesthetics. This is not the same and in the next section we show by simulation that the differences between designometric and psychometric perspectives can be dramatic.

Recall that psychometric validations require large samples of participants! When swapping roles, validation studies that included multiple, but only a few, designs, may not be repairable. One example is the study by @Ho2010 validating the Godspeed Index, a common multi-scale inventory to evaluate robot designs. Their designometric study included 38 items and 30 participants, but only 12 designs. While they did not specify how the designometric box was collapsed, the fact that they were still able to report exporatory factor analysis results, suggests that they used a psychometric response matrix, as the designometric matrix would have been too small. To be fair, the study tested validity correctly comparing designs, although with simple ANOVA models.

As a milder form *run-time psychometric fallacy* appears when an existing instrument is used in practice to take measures on a single design. The result will inevitable look like a psychometric response matrix and, given that publication rules often require to report test reliability, it may be tempting for the researcher to run a psychometric test. While the run-time fallacy does not have the same impact as development-time fallacies, it may cause confusion when a validated instrument seems to have poor reliability.

```{r setup}
library(tidyverse)
library(psych)
library(mascutils)
library(printr)
#library(lavaan)
options(mc.cores = 8)

purp.analysis <- T



```

```{r}
#' Psychometric/designometric response matrix from long designometric data
#' 
#' @param x long designometric data
#' @returns psychometric response matrix.
#' @examples
#' ldmx <- expand.grid(Design = 1:7, Part = 1:5, Item = 1:3)
#' ldmx$response <- rbeta(105, 2, 2)
#' rm_pmx(ldmx)
#' rm_dmx(ldmx)

rm_pmx <- function(x)
  x |> 
  group_by(Part, Item) |> 
  summarize(mean_resp = mean(response)) |> 
  ungroup() |> 
  arrange(Item) |> 
  spread(Item, value = mean_resp) |> 
  select(-Part)

#' @rdname rm_pmx
rm_dmx <- function(x) 
  x |> 
  group_by(Design, Item) |>
  summarize(mean_resp = mean(response)) |> 
  ungroup() |> 
  spread(Item, value = mean_resp) |> 
  select(-Design)


#' Psychometric function wrappers
#' 
#' @param Data long designometric data
#' @returns Results per Perspective and Scale

alpha_ci <- function(Data){
  Scale <- str_c(distinct(Data, Scale)$Scale) 
  model_psych <- 
    psych::alpha(rm_pmx(Data), check.keys = FALSE, n.iter = 100)$boot |> 
    as_tibble() |> 
    mutate(Perspective = "psychometric")
  model_design <- 
    psych::alpha(rm_dmx(Data), check.keys = FALSE, n.iter = 100)$boot |> 
    as_tibble() |> 
    mutate(Perspective = "designometric")
  out <- 
    bind_rows(model_psych,
              model_design) |> 
    select(Perspective, std.alpha) |> 
    group_by(Perspective) |> 
    summarize(center = mean(std.alpha),
              lower = quantile(std.alpha, .025),
              upper = quantile(std.alpha, .975)) |> 
    mutate(Scale = Scale) |> 
    mascutils::go_first(Scale, Perspective)
  out
}

item_rel <- function(Data){
  #Data <- D_1 |> filter(Scale == "HQI")
  Scale <- str_c(distinct(Data, Scale)$Scale)
  model_psych <- 
    psych::alpha(rm_pmx(Data), check.keys = FALSE)$item.stats |> 
    as_tibble(rownames = "Item") |> 
    mutate(Perspective = "psychometric")
  model_design <- 
    psych::alpha(rm_dmx(Data), check.keys = FALSE)$item.stats |> 
    as_tibble(rownames = "Item") |> 
    mutate(Perspective = "designometric")
  
#  model <- M_1_design
  out <- 
    bind_rows(model_psych,
              model_design) |> 
    mutate(Scale = Scale) |> 
    go_first(Scale, Item, Perspective) |> 
    arrange(Scale, Item, Perspective)
  out
}

parallel_analysis <- function(Data, n, persp, scales){
  if (persp == "D") {
    data <- rm_dmx(Data)
    main <- str_c("Designometric Parallel Analysis of ", scales)
  }
  if (persp == "P") {
    data <- rm_pmx(Data)
    main <- str_c("Psychometric Parallel Analysis of ", scales)
  }
  psych::fa.parallel(data,
                   fa = "fa",
                   fm = "minres",
                   nfactors=n,
                   main=main)
    
}
```

# Simulation study

The following example demonstrates the difference by simulating a situation, where a fictional three-item scale for Coolness is highly reliable for persons, but seemingly has no reliability at all for discerning the tested designs. Such a pattern can occur when the design sample bears little variance with respect to the property in question. In the following simulation, we assume that the Coolness scale has been tested on a sample of 50 premium law firm home pages and 50 participants of various ages and social background.

The simulation uses zero-centered Normal distributions to draw the parameters for 20 design, 20 participant and 4 items. Subsequently these are combined into responses $R = D + P - I$ with some extra noise ($\sigma_\textrm{Part} = .5$). The key here is that items and participants vary strongly in their appreciation of Coolness ($\sigma_\textrm{Part} = \sigma_\textrm{Item} = .2$), whereas the sample of designs varies much less in Coolness ($\sigma_\textrm{Design} = .05$).

```{r}
#| include: false
set.seed(42)

n_Design = 20
n_Part   = 20
n_Item  =  4
n_Obs = n_Design * n_Part * n_Item

Designs <- tibble(Design      = as.factor(1:n_Design),
                  cool_Design = rnorm(n_Design, 0, .05)) ## low level, little variation

Parts   <- tibble(Part        = as.factor(1:n_Part),
                  cool_Part   = rnorm(n_Part, 0, .2)) ## strong variance in tendency to judge sth. cool

Items   <- tibble(Scale       = "Coolness",
                  Item        = as.factor(1:4),
                  cool_Item   = rnorm(n_Item,  0, .2)) ## item strength: understating items get lower values

Coolness     <- expand_grid(Design = Designs$Design,
                       Part   = Parts$Part,
                       Item   = Items$Item) |> 
  left_join(Designs) |> 
  left_join(Parts) |> 
  left_join(Items) |> 
  mutate(response = mascutils::rescale_zero_one(cool_Design + cool_Part - cool_Item + rnorm(n_Obs, 0, .5)))

Rel_Coolness <- alpha_ci(Coolness)
```

```         
```

```{r}
#| label: tbl-simulated-reliability
#| tbl-cap: "Reliability under both perspectives using simulated data (fictional Coolness scale)"

Rel_Coolness |> knitr::kable(digits = 2)
```

This simple example demonstrates that psychometric reliability (person sensitivity) can be excellent ($.93$, see @tbl-simulated-reliability), whereas designometric reliability is poor ($.60$). Under the psychometric fallacy it is possible that excellent reliability is reported, while it is actually very poor. In the following study we examine how severe the psychometric fallacy is in real practice.

# Methods

In order to assess the biases introduced by the psychometric fallacy, a secondary data analysis was conducted using data from seven prior experiments, which were originally testing original hypotheses on User Experience and Human-Robot Interaction (@tbl-data-summary). What was common is that data was obtained in complete designometric encounters, with large samples of designs.

In QB, JK, SP and DN participants saw screen shots of home pages and responded to several user experience scales, whereas in AH, DK and PS the stimuli were robot faces. All experiments used manipulation of presentation time to collect data on subconscious cognitive processing. For the analysis here, presentation times lower than 500ms were discarded. All experiments used a (mildly) incomplete design in that participants encountered all designs and items several times, but not in every possible combination.

```{r}
#| include: false
load("DMX_data.Rda")
D_1 <- D_1 |> 
  mutate(Study = if_else(Study == "QB" & Scale == "Credib", "JK", Study))
```

```{r}
#| label: tbl-data-summary
#| message: false
#| warning: false
#| paged-print: true
#| tbl-cap: "Summary of data sets used for analysis"

D_1 |> 
  group_by(Study, Scale) |> 
  summarize(n_Design = n_distinct(Design),
            n_Items = n_distinct(Item),
            n_Part = n_distinct(Part),
            n_Obs = n()) |> 
  ungroup()  |> 
  mutate(Reference = stringr::str_c("@", Study)) |>
  arrange(Scale) |> 
  mutate(Designs = if_else(Scale == "nEeriness", "Robot faces", "Homepages")) |> 
  knitr::kable()
```

## Item and and Design samples

In total, eight rating scales were applied to four design samples.
The bipolar *Eeriness* scale (already mentioned above) is a primary research tool on the Uncanny Valley phenomenon and measures negative emotional responses towards artificial faces. Factor structure and reliability were originally established under psychometric perspective, whereas experimental validation correctly compared designs [@Ho2017]. AH used morphing levels between human and robot faces as stimuli, whereas the other two experiments used a subset of @Mathur2016, with a few new designs added.

All other scales were applied a sample of commercial home pages collected by @Tuch2012 (a subset in DN):

-   The *Attractiveness* scale is unipolar subscale of the User Experience Questionnaire (UEQ) inventory and has undergone basic psychometric evaluation in six studies with a single design each [@Laugwitz2008].

-   The two 7-item bipolar scales *Hedonic Quality - Identity (HQI)* and *Hedonic Quality - Stimulation (HQS)* are from the AttrakDiff2 inventory, which underwent primary psychometric validation on three designs [@Hassenzahl2003]. The scales *Hedonism* and *Usability* in @DN were taken from the short version of AttrakDiff2, but will be considered separately in this analysis.

-   @DN composed the Beauty scale from the item used in @Hassenzahl2010 and three items representing classic aesthetics from @Tractinsky2006.

-   The *Credibility* scale was originally designed to compare people's attitude towards media (newspapers, TV, radio). The validation study used exploratory factor analysis for psychometric item selection with 1468 participants [@Gaziano1986].

## Apparatus

@DN used OpenSesame for stimulus presentation and implemented the collection as graded responses [@OpenSesame]. All other experiments used the same PsychoPy program and collected continuous responses using a visual analog scale [@Peirce2008].

## Participant samples

Participants were sampled by convenience with sizes between 25 and 45 and a strong over-representation of university-level Social Sciences students and associated circles.

```{r}
#| eval: false
D_1 |> 
  ggplot(aes(x = response)) +
  geom_histogram() +
  facet_wrap(~Scale, scale = "free_y")
```

```{r}
#| eval: false
D_Eer |> 
  ggplot(aes(x = Item, y = response, color = Study)) +
  geom_violin()
```

```{r}
#| eval: false
D_1 |> 
  distinct(Study, Design) |> 
  group_by(Study) |> 
  summarize(n_Design = n()) |> 
  ungroup() |> 
  left_join(D_1 |> 
              distinct(Study, Part) |> 
              group_by(Study) |> 
              summarize(n_Part = n())|> 
              ungroup()) |> 
  left_join(D_1 |> 
              group_by(Study) |> 
              summarize(n_Obs = n())|> 
              ungroup()
  )  |> knitr::kable()
```

```{r}
#| eval: false

D_1 |> 
  distinct(Scale, Design) |> 
  group_by(Scale) |> 
  summarize(n_Design = n()) |> 
  ungroup() |> 
  left_join(D_1 |> 
              distinct(Scale, Part) |> 
              group_by(Scale) |> 
              summarize(n_Part = n())|> 
              ungroup()) |> 
  left_join(D_1 |> 
              group_by(Scale) |> 
              summarize(n_Obs = n())|> 
              ungroup()
  )  |> knitr::kable()
```

## Data Analysis

Goal of the analysis is to examine how the psychometric fallacy compromises the evaluation of rating scales. For this purpose, the data sets were separately collapsed into psychometric and designometric response matrices. Subsequently, three basic psychometric techniques were applied to both perspectives and compared: Scale reliability, item consistency and number of factors.

For all three procedures functions from the well-established R package Psych [@Psych] were used. *Scale reliability* and *item consistency* evaluations were conducted using Cronbach $\alpha$ and derived from that the corrected item-total correlations (`psych::alpha`).

*Number of factors* was identified using parallel analysis with `psych::fa.parallel` (set to a factor-analytic model fitted by minimizing the residuals). This produces an eigenvalue obtained on real data and compares it to eigenvalues obtained from simulated data obtained through random resampling. A factor model is retained, when its eigenvalue exceeds the simulated eigenvalue. The number of factors is determined as the point before the the real eigenvalue drops below the siomulated level.

# Results

## Scale reliability

```{r}
#| include: false
Scale_rel <-
  D_1 |> 
  group_by(Scale) |> 
  group_split() |> 
  map_df(alpha_ci)
```

```{r}
#| label: fig-scale-reliability
#| fig.height: 8
#| fig.width: 8
#| fig-cap: "Cronbach alpha item-level reliability estimates compared by perspective and scale"

Scale_rel |> 
  ggplot(aes(color = Scale,
             label = Scale,
             x = Perspective,
             y = center,
             ymin = lower,
             ymax = upper)) +
  geom_point() +
  geom_line(aes(group = Scale)) +
  ylab("std. Cronbach alpha") +
  geom_label() +
  ylim(0,1)
```

Overall scale reliabilities cover a broad range from excellent to unusable [@fig-scale-reliability]. All scales improve under the designometric perspective, albeit, the difference ranges from barely noticable (HQS, HQI) to very strong (Hedonism, Usability, Beauty and Attractiveness). The most dramatic difference can be seen in Hedonism and Beauty, which both have excellent designometric reliability, but unusable otherwise.

## Item consistency

@fig-item-reliability shows item consistency as corrected item-total correlations as a measure for item consistency. Beauty and Hedonism stand out, because all items take a similar sharp drop in psychometric reliability. To some extent this also seems to hold for Usability and Eeriness. For Credibility, HQ-I, HQ-S and Attractiveness some items drop under psychometric perspective, whereas others improve, with an extreme cases: Item  Att6 is already on a very low level on designometric reliability, becoming even negatively correlated under psychometric perspective. Items HQI5 and HQI6 designometric performance is poor, but are among the overall best performing psychometric items.

```{r}
#| include: false
Item_rel <-
  D_1 |> 
  group_by(Scale) |> 
  group_split(Scale) |> 
  map_df(item_rel)

```

```{r}
#| label: fig-item-reliability
#| fig.height: 8
#| fig.width: 8
#| fig-cap: "Cronbach alpha item-level reliability estimates compared by perspective and scale"

Item_rel |> 
  ggplot(aes(x = Perspective,
             y = r.cor)) +
  geom_line(aes(group = Item)) +
  ylab("Item-total correlation") +
  geom_label(aes( label = Item)) +
  facet_wrap(~Scale, ncol = 2) +
  geom_point(data = rename(Scale_rel, alpha = center),
             aes(x = Perspective, 
                 y = alpha,
                 col = "Scale reliability")) +
  geom_line(data = rename(Scale_rel, alpha = center),
             aes(x = Perspective, 
                 y = alpha,
                 group = Scale,
                 col = "Scale reliability"))


```

## Number of factors

Given a response matrix, the number of factors were estimated using parallel analysis. Ideally, this procedure returns exactly as many factors as there are separate scales in every data set.

<!-- Item reliability measures how coherent any particular item is with the scale as a whole. This criterion regularly used during item selection, where an item may be discarded if it does not correlate with the remaining items. However, if some items are out-of-tune, this can mean that they represent a different facet of the construct under examination.  -->

<!-- Exploratory factor analysis is frequently used to find such factors in response matrices. The first step typically is to determine the number of factors. -->

```{r}
#| include: false

# Running parallel analysis over data sets and perspectives. One list object per data set.

PA_Eer <- list(dmx = parallel_analysis(D_Eer, 2, "D", "Eeriness"),
                pmx = parallel_analysis(D_Eer, 2, "P", "Eeriness"))


PA_Att <- list(
  dmx=parallel_analysis(D_Att, 3, "D", "AttrakDiff and Credibility"),
  pmx=parallel_analysis(D_Att, 3, "P", "AttrakDiff and Credibility"))

PA_HUB <- list(
  dmx=parallel_analysis(D_HUB, 3, "D", "Hedonism, Usability and Beauty"),
  pmx=parallel_analysis(D_HUB, 3, "P", "Hedonism, Usability and Beauty"))

str(PA_Eer$dmx)


plot.nfactors <- function(x) {
  #psych:::plot_fa.parallel(x, main=NA)
  n_dmx <- x$dmx$nfact
  n_pmx <- x$pmx$nfact
  n_factors <- tibble(Perspective = c("designometric", "psychometric"), 
                     data = c(x$dmx$fa.values[n_dmx], x$pmx$fa.values[n_pmx]),
                     N = c(n_dmx, n_pmx))
  bind_rows(
    tibble(
      N = 1:length(x$dmx$fa.values),
      data = x$dmx$fa.values, 
      sim = x$dmx$fa.sim, 
      Perspective = "designometric"),
    tibble(
      N = 1:length(x$pmx$fa.values),
      data = x$pmx$fa.values,
      sim = x$pmx$fa.sim,
      Perspective = "psychometric")) |> 
  ggplot(aes(x=N, col = Perspective)) +
    geom_line(aes(y = data, lty = "data")) +
    geom_line(aes(y = sim, lty = "simulated")) +
    ylab("Eigenvalue") +
    xlab("number of factors") +
    geom_hline(yintercept = 1)+
    geom_label(data = n_factors, aes(x = N, y = data, label = N), nudge_y = .2)

}
```

The Eeriness scale is part of a larger Godspeed Index inventory and is supposed to represent a single latent variable. However, @Ho2017 found that the scale decomposes into two slightly different aspects, summarized as "eerie" and "spine-tingling". This was established using principal component analysis, whereas a dedicated identification of the number of factors has not been reported. Since this was tested with only 12 designs, most likely it was under psychometric perspective. The results in show that for both perspectives the eigenvalue drops below the simulated eigenvalue with two factors under both perspectives (@fig-nfactors-eeriness).

```{r}
#| label: fig-nfactors-eeriness
#| fig-cap: "Number of factors under designometric and psychometric perspectives for the Eeriness scale using parallel analysis"
#| fig-width: 5
#| fig-height: 4

plot.nfactors(PA_Eer) + ylim(0,6) + xlim(1,8) + ggtitle("Eeriness scale")
```

On theoretical grounds, the AttrakDiff2 inventory splits hedonistic quality into two components, Identity and Stimulation, while the credibility scale is a completely separate construct. We would expect three factors to emerge. As @fig-nfactors-att shows the two perspectives deviate in opposite directions: For the psychometric perspective, the eigenvalues drop below their simulated counterparts at two factors, whereas for the designometric perspective stays above this line with five factors.

```{r}
#| label: fig-nfactors-att
#| fig-cap: "Number of factors under designometric and psychometric perspectives for the AttrakDiff  inventory using parallel analysis"
#| fig-width: 5
#| fig-height: 4

plot.nfactors(PA_Att) + ylim(0,6) + xlim(1,8)+ ggtitle("AttrakDiff and Credibility")
```

Finally, in study DN three independent scales, Hedonism, Usability and Beauty, were used und we expect three factors. In contrast, the eigenvalues drop below even the simulated eigenvalues with two factors, suggesting that the same latent variable is captured by all three scales under both perspectives (@fig-nfactors-hub).

```{r}
#| label: fig-nfactors-hub
#| fig-cap: "Number of factors for Hedonic value, Usability and Beauty using parallel analysis"
#| fig-width: 5
#| fig-height: 4

plot.nfactors(PA_HUB) + ylim(0,6) + xlim(1,8) + ggtitle("Hedonism, Usability and Beauty")
```


# Discussion

Rating scales in Human Factors research are commonly used to discriminate between poor and good design options, rank designs, choose designs, or perform UX regression tests in continuous design cycles. Our logical argument is that the capability of a scale to rank designs can only be seen on multiple designs and using design-by-item response matrices. We called it the psychometric fallacy to use person-by-item response matrices in place. A simulation showed, that the worst case can happen under the psychometric fallacy: excellent reliability is reported, when it actually is poor.

With data from five experiments we showed that the psychometric fallacy is real and produces biases across the board, sometimes dramatic. In many cases, scale reliability is very different between the two perspectives, with designometric reliability being generally higher. While the differences in reliability were large across scales, on designometric level they were all in a useful range, ranging from just useful (Credibility) to excellent (Hedonism, Beauty). But, many differences exist on item level, going in both directions. Accordingly, the factor cardinality differs from theoretical expectations in all but one cases (Eeriness).

In the following we discuss the details and implications of our findings for scale developers and users, before we outline an agenda for more advanced (deep) designometric methods.

## Implications for scale development

In design research the target of all research is quickly changing and expanding target. A certain swiftness and pragmatism is required to keep up with the pace. Development of new scales is a common task, and often it is carried out by researchers with a basic understanding of psychometric principles, such as (item) reliability and exploratory factor analysis.

Basic psychometric tools produce vastly different results under the psychometric fallacy. While our study used mature scales, which had already undergone item selection and perhaps factor analysis, we can interpolate the consequences for future scale development.

The most severe consequence is that a scale may be developed that is not capable of ranking designs. According to an often cited rule-of-thumb, scale reliability should be at least .7. Three scales in our study, Attractiveness, Credibility and HQS did not meet this criterion, even under the designometric perspective.

The HQ-I scale (and to some extent also Credibility) shows a concerning pattern, where some items perform well psychometrically, but are designometrically extremely weak. This shows that developing a designometric scale under psychometric perspective can lead to *falsely favored* items that are well-behaved in ranking persons, but are inefficient for designs. To make the case, scale reliability when removing item HQI5 improves to $0.73$ compared to $0.69$. By further removing HQI1, reliability is $0.68$. While this is not a major increase in reliability, the same level is effectively reached with fewer items - the psychometric fallacy can lead to inefficient scales.

```{r}
#| label: rel-after-removal
#| caption: Designometric reliability of Attractiveness, Hedonism and Credibility before and after removal of falsely favored items.
#| include: false

  D_1 |> 
  filter(Scale %in% c("HQI", "Credib")) |> 
  filter(!Item %in% c("Credib1", "Credib2", "HQI5")) |> 
  group_by(Scale) |> 
  group_split() |> 
  map_df(alpha_ci)

```

While we cannot show that directly, it is likely that fallacy also leads *falsely reject* items that are actually well-behaved in ranking designs. Creating an item pool is by itself a time-consuming process, and the psychometric fallacy can make it even more difficult by rejecting good items and selecting inefficient ones. A recent example is the development of the BUS-11 scale, where face validity demands (and factor analysis has confirmed) that *Privacy* is a separate construct, only one item was left after item selection under psychometric perspective [@Borsci2022].

## Implications for users

For practitioners, the good news are that if they were under the run-time psychometric fallacy by routinely reporting scale reliability, they were always better than they said. And when they continue to use these scales in the future, the improved precision will allow them to reduce sample sizes.

But, practitioners may not yet have the most efficient rating scales. Even if a false favored item is not directly harming reliability, it can make the scale inefficient. In practice, UX scales are often deployed during use, for example in usability tests. With a shorter scale measures can be taken in quicker succession, for example once per task, or everyday in a longitudinal study. It is therefore not uncommon for practitioners to create a reduced scale, for example, when many latent variables are involved. For some scales (Hedonism, Beauty) it is safe to just pick three items at random. Other scales are quite mixed bags, with the highest ranked item under psychometric perspective being the lowest ranked designometrically.

```{=html}
<!--Past users may have scratched their heads more often than we know. Techniques such as Cronbach $\alpha$ are commonly applied as quick sanity checks, whenever multi-item scales are used in run-time studies. Everyone in the past who used designometric scales and followed APA guidelines, was likely to the psychometric fallacy and experience some unnecessary headaches.

With better item-level reliability, experiments can also be streamlined by using a reduced set of items. Reducing a scale is basically item selection and several strategies are possible, as long as it is carried out on a design-by-item matrix.

A basic strategy is to use item-level reliability a ranking criterion. In many cases this may work well, but takes the risk of inadvertently reducing the precision in certain ranges of the scale. A well-constructed scale produces similarly precise measures in the lower, middle and high range on the whole scale. During development time this can be achieved by selecting items that are varying in strength, like "The interface is beautiful." is stronger than "... pleasant". By taking the average score-->
```

<!-- ### Criticism of individual scales -->

<!-- One can rightfully argue that we used the rating scales outside their specification, as the encounters were brief and without interaction. This depends on the scale and on the purpose. Face processing is one of the fastest complex mechanisms in the human mind, which makes it legit to test it in quick succession on a screen. Similarly, the beauty judgement of a website is known to stabilize within 500ms, and the same can be expected for attractiveness. For more longitudinal feelings about designs, like hedonism, usability and credibility, these encounters were not valid. We will therefore not criticize these instruments, the point still standing that the two perspectives produce different results. -->

<!-- -   Eeriness -->

<!-- -   Beauty -->

<!-- -   Attractiveness -->

<!-- TODO -->

## Applications

A key idea in usability engineering is that interaction designers learn to bridge the gap between the system model and the users mental model, cognitive skills and feelings. Emerging technologies are often characterized by an innovation phase, where multiple design paths are explored in a rush to the market and several domains of human-technology interaction are currently gaining momentum: large language model technology is, as of writing, causing much attention for intelligent agent design. Humanoid and animalistic robot design is coming out of its niche, and virtual reality applications are already mainstream. These three domains have in common that, compared to classic computer applications, they are tapping into new territories of the users mind, the social mind and the sensation of physical reality.

When Social Experience (SX) or Virtual Experience (VX) become the new UX, it may start with the same abundance of new instruments trying to map the uncharted design space. These are huge and effective designometric instruments are needed to guide design decisions in hyper-excited times [@hypecycle2023]. Our results show, that the psychometric fallacy is harmful during scale development, leading to inefficient item sets and factor structures.

## Towards Deep Designometrics

By comparing the two perspectives, we illustrated that designometrics can be accomplished with standard psychometric tools by flattening the response box across participants. However, by averaging across participants, we loose all information on users. It would even be possible to evaluate a designometric model on the responses of a *single user*. Formally, this would be a valid designometric measure, reflecting a single person's sensitivity to design features.

Designometric scales are commonly used to measure a populations reaction to a design, which implies that on some level the psychometric matrix is useful, for example to study the distribution of user sensitivity to a feature. Imagenable cases exist where one could use a designometric scale for psychometric purposes. For example, an instrument to measure trustworthiness of designs could be used to estimate faithfulness of participants in a study (or a training) on cyber security.

By flattening the designometric box one way, then the other, we still loose information that is needed to secure that items are truly well-behaved. In educational psychometrics *differential item functioning* is the idea that items must be fair and function the same for every tested person. This is a desirable property for a designometric scale, but a statistical model for verification would need individual parameters for participants, designs and items, simultaneously. @Schmettow2021_multilevel proposed multi-level models for capturing designometric situations in their full dimension, which could be well-suited for run-time use or basic scale development.

Such a multi-level approach is well in line with the established field of generalizability theory [@Brennan2001]. In that approach, the variance in responses is partitioned into multiple sources. Thus, suppose we have a data collection design where multiple users judge several designs, on multiple items, the variance in responses can be partitioned into variance due to individual differences in users, differences in designs, and differences in items, This can be done for the response box, but can straightforwardly be extended to hyperboxes, for instance having data collected from the same people on a set of tasks Another consideration is that the designometric encounter may not be end of story. For example, for comparing multi-purpose designs a researcher may want to add tasks as fourth population of interest [@Schmettow2013s].

Such variance decomposition can be implemented in multi-level models, where the estimated variances can be used to compute reliability, whether it relates to measuring differences in users or differences in designs, while controlling for all other relevant sources of variation. By extending the modelling to multi-level IRT models [@Fox2001, @vandenBerg2007] the discrete nature of the item response data can be taken into account.

While multi-dimensional exploratory methods have been well developed in chemometrics and sensory science [@Tucker1966, @Harshman1970], they’ve seen little integration into mainstream psychometric or UX-scale validation workflows. Bridging this gap—by extending factor-analytic methods to multi-dimensional designometric data—constitutes a critical next step in establishing a rigorous quantitative foundation for design measurement.

Finally, the Eeriness scale is proof that *universal rating scales* are possible, which robustly perform in more than one perspective. Understanding how to develop such scales is an important next step in developing a deeper designometric methodology.

# References
