% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  authoryear,
  preprint]{elsarticle}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{elsarticle-harv}


\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{PsyArxiv}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Designometric Perspective and the Psychometric Fallacy in Developing User Experience Rating Scales},
  pdfauthor={Martin Schmettow; Simone Borsci; Stéphanie M. van den Berg},
  pdfkeywords={Psychometrics, User Experience, Human Factors},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{The Designometric Perspective and the Psychometric Fallacy in
Developing User Experience Rating Scales}
\author[1]{Martin Schmettow%
\corref{cor1}%
}
 \ead{m.schmettow@utwente.nl} 
\author[1]{Simone Borsci%
%
}
 \ead{s.borsci@utwente.nl} 
\author[1]{Stéphanie M. van den Berg%
%
}
 \ead{stephanie.vandenberg@utwente.nl} 

\affiliation[1]{organization={University of Twente},city={Enschede,
NL},postcodesep={}}

\cortext[cor1]{Corresponding author}



        
\begin{abstract}
Rating scales are widely used in user experience (UX) research to
compare and rank design alternatives, yet their development typically
relies on psychometric procedures originally created for assessing human
traits. We argue that this practice commits the psychometric fallacy:
evaluating designometric instruments---tools intended to discriminate
between designs---using person × item response matrices that exclude the
essential third dimension of design. Because design evaluation
inherently forms a design × person × item data structure, valid scale
development requires samples of designs large enough to assess how well
items rank design alternatives. We show that collapsing the data cube
along persons yields a design × item matrix that allows psychometric
tools to be applied meaningfully, whereas collapsing along designs
yields misleading psychometric results about person sensitivity rather
than design discriminability. A simulation study demonstrates that
scales can appear highly reliable psychometrically while being
effectively unusable for ranking designs. Secondary analyses of eight
commonly used UX scales across large design samples reveal unsystematic
distortions in reliability estimates, item performance, and
dimensionality when instruments are evaluated under the psychometric
fallacy. We assess the impact of the psychometric fallacy for present
users and designers of future rating scales and discuss more advanced
methods in designometric modelling.
\end{abstract}





\begin{keyword}
    Psychometrics \sep User Experience \sep 
    Human Factors
\end{keyword}
\end{frontmatter}
    

\section{Introduction}\label{introduction}

Use experience rating scales constitute a fundamental measurement
instrument in contemporary industrial applications, providing
cost-effective and readily accessible methods for comparative evaluation
and benchmarking of design artifacts. The utility of rating scales in
decision-making processes is contingent upon critical statistical
properties of the scale, such as item consistency, reliability and
factor structures.

The development of effective rating scales is a methodologically
rigorous endeavor. Psychometrics, the scientific discipline concerned
with the quantitative assessment of psychological attributes to enable
comparative evaluation of individual performance and functioning, has
traditionally focused on cognitive abilities such as mathematical
reasoning and linguistic comprehension. Subsequently, the field expanded
to encompass measurement of latent psychological constructs, including
personality dimensions (e.g., the Five-Factor Model).

Following the rapid expansion of user experience (UX) research,
\citet{Bargas-Avila2011} documented the proliferation of hundreds of
novel rating scale instruments. However, the majority of these
instruments have not undergone the rigorous psychometric validation
procedures typically required for clinical or educational assessment
tools. Nevertheless, psychometric methodologies are occasionally
employed during instrument development phases, particularly for
reliability estimation and sub scale identification through factor
analysis. Additionally, practitioners using designometric instruments
may conduct preliminary evaluations to ensure data quality and
integrity.

The central theoretical argument in this investigation posits that
design research (aka \emph{designometric}) instruments, specifically
rating scales, function primarily to establish rank orderings among
design alternatives. Consequently, the development of designometric
instruments necessitates evaluation across extensive design samples to
assess their ranking capabilities. This requirement results in a
three-dimensional data structure (design × person × item), which is an
extension of the two-dimensional response matrices (person × item) used
in traditional psychometric analyses.

The phenomenon of the ``psychometric fallacy'' occurs when designometric
rating scales are evaluated using conventional psychometric response
matrices (person × item). This approach contradicts the fundamental
requirement that design samples must be incorporated into the validation
process. Numerous purported designometric instruments have been
developed under this fallacy, potentially compromising their capacity to
effectively discriminate among design alternatives.

The practical contribution of this work demonstrates that psychometric
analytic tools remain applicable to designometric data through
dimensional reduction of the three-dimensional array into a design ×
item response matrix. This transformation constitutes solely a semantic
reinterpretation, whereby statistical measures originally developed for
ranking individuals are reapplied to ranking design artifacts.

Given that psychometric analytic tools typically require two-dimensional
input matrices, and designometric data cubes can be collapsed along
either dimension to yield either designometric (design × item) or
psychometric (person × item) matrices, the implications of the
psychometric fallacy can be systematically investigated through
comparative analyses using both matrix configurations.

This investigation establishes the designometric measurement perspective
and demonstrates the appropriate application of fundamental psychometric
tools for both the development and practical implementation of
designometric scales. The consequences of the psychometric fallacy are
further examined through secondary analysis of designometric data
collected using eight widely employed UX rating scales.

In the following section we present a short recap on psychometric
principles and workflows, after which we derive principles of proper
designometric scale development and discuss the psychometric fallacy.
While we believe that the theoretical argument can stand on its own, we
next present a small simulation study to show the mechanics, followed by
an illustration of the real-world consequences of the psychometric
fallacy on empirical data, obtained from several commonly used UX rating
scales. Last, we discuss more sophisticated methods of dealing with
design evaluation data.

\subsection{Principles of Psychometric Test
Development}\label{principles-of-psychometric-test-development}

Psychometric instruments serve primarily as cost-effective predictive
tools for human performance and psychological attributes.
Psychodiagnostic rating scales function as screening instruments for
conditions such as depression \citep{Kroenke2001}, while performance
assessments facilitate decision-making in personnel selection and
educational contexts \citep{Schmidt1998}.

Three psychometric schools of thought provide the theoretical framework
under which multi-item instruments yield reliable individual rankings:
Classical Test Theory (CTT) focuses mainly on the aspect of error
reduction through repetition, whereas Item Response Theory (IRT) adds
more rigor to item selection and Factor Analysis (FA) provides support
for multidimensional constructs.

\subsubsection{Item Selection and Scale
Development}\label{item-selection-and-scale-development}

Psychological assessments necessitate multiple items due to inherent
measurement error across self-report scales, reaction time measures, and
physiological indicators. The central theoretical insight of CTT is that
taking repeated measures improves measurement precision through error
reduction, as formalized by the Law of Large Numbers. In practice, this
is achieved by using sets of items.

Multi-item scale construction begins with comprehensive domain analysis
to identify relevant psychological processes and behavioral dimensions.
Following domain specification, researchers develop extensive item pools
that typically exceed the target scale length. This initial phase
employs qualitative, divergent methodologies emphasizing content
validity and theoretical coverage \citep{Hinkin1998}.

Subsequent item selection procedures employ quantitative criteria
including item-total correlations, factor loadings, and reliability
coefficients. The iterative selection process aims to optimize
measurement accuracy while maintaining domain representation. According
to CTT principles, measurement errors across multiple items
cancel--provided systematic variance components demonstrate strong
intercorrelation \citep{Cronbach1951}.

Cronbach's \(\alpha\) quantifies internal consistency by measuring
inter-item agreement within the response matrix \citep{Cronbach1951}.
Item selection procedures compare full-scale reliability against
reliability estimates with individual items removed. Items whose removal
improves overall reliability are identified for elimination. Similar
procedures examine item-total correlations to identify poorly performing
indicators \citep{Clark1995}.

\subsubsection{Factor Structures and Latent Variable
Models}\label{factor-structures-and-latent-variable-models}

Step-wise item removal procedures prove adequate for unidimensional
constructs but may produce unstable results when the measured domain is
more complex. Factor-analytic methods therefore serve to identify and
separate distinct components into psychometrically sound subscales
\citep{Fabrigar1999}.

Contemporary psychometric theory distinguishes between latent variables
(unobservable true scores) and indicator variables (observable but
imperfect measurements). Complex instruments often incorporate multiple
latent variables representing distinct domain aspects. As an example,
the Five-Factor Model proposes that personality can be assessed through
five primary traits, each measured via multi-item subscales
\citep{McAdams1992}. While primary factors demonstrate relative
independence by design, subscales within factors exhibit stronger
intercorrelations.

Domain analysis often suggests potential factor structures, particularly
when theoretical frameworks guide instrument development. For example,
mental workload assessment scales may derive from Multiple Resource
Theory, which predicts independent processing of sensory channels,
thereby supporting a separate subscale for each channel
\citep{Wickens2002}.

When theoretical structures exist a prior, Confirmatory Factor Analysis
(CFA) provides the optimal approach for testing structural assumptions
about latent variable relationships \citep{Brown2015}. Hierarchical CFA
models can verify independence among primary scales while confirming
stronger correlations among subscales.

Exploratory Factor Analysis (EFA) serves to identify novel factor
structures when robust theoretical frameworks are unavailable
(\citet{Costello2005}). EFA requires researchers to specify the number
of factors and their correlation structure. This is inconvenient because
an unknown factor structure implies an unknown number of factors.

Finding the number of factors is possible by successively increasing the
number of factors as long as a factor retention criterion is met. Common
criteria include the Kaiser-Guttman rule (eigenvalues \textgreater{} 1)
and scree plot inspection, but these have been criticized for being to
lenient and subjective \citep{Hayton2004}. A more accurate alternative
is parallel analysis, which compares observed eigenvalues to those
obtained from data of identical size but randomized through resampling
\citep{Lim2019}.

Another decision to make is factor rotation which depends on theoretical
expectations: orthogonal rotation applies when components are
independent (e.g., mathematical and verbal abilities), while oblique
rotation accommodates correlated factors typical of subscales
\citep{Fabrigar1999}.

\subsubsection{Response Matrix Structure and Item Response
Theory}\label{response-matrix-structure-and-item-response-theory}

Traditional psychometric methods operate on response matrices with
participants as rows and items as columns. Standard procedures include
computing reliability estimates on complete matrices, evaluating
consistency improvements following item removal, and conducting factor
analyses on participant × item data structures.

Item Response Theory (IRT) represents an alternative framework that
treats response matrices as collections of person-item encounters
\citep{Embretson2013}. Unlike CTT, IRT models both person and item
parameters simultaneously, allowing formal specification and empirical
testing of item characteristics. The Rasch model, representing the
simplest case of unidimensional and dichotomous responses, specifies
that response probability depends solely on the difference between
person ability and item difficulty \citep{Rasch1960}. Advanced IRT
applications include differential item functioning detection to identify
and prevent measurement bias across demographic groups
\citep{Penfield2000}.

\subsubsection{Sampling Requirements in Psychometric
Development}\label{sampling-requirements-in-psychometric-development}

Irrespective of theoretical orientation or analytic sophistication, all
psychometric instrument development activities center on establishing
psychometrically sound item sets. The substantial participant samples
required throughout this developmental process represent one of the most
challenging aspects of psychometric research. These requirements stem
from several converging methodological imperatives.

To begin with, samples must sufficiently represent the population in
question, with the general rule that heterogeneous populations require
larger sample sizes. During statistical analysis, sample size must at
least match the number of free parameters in the analytic model to
ensure identifiability, but in practice this is usually not sufficient
\citep{Bollen1990}. For example, in Confirmatory Factor Analysis, each
item contributes two free parameters (intercept and factor loading),
requiring participant-to-parameter ratios of 5:1 to 20:1
\citep{Brown2015}. Contemporary simulation studies suggest 200-500
participants typically suffice for well-specified models with strong
factor loadings, while complex structures may require 1000 or more
participants \citep{Wolf2013}.

When theoretical structures are absent, Exploratory Factor Analysis can
be used to identify suitable subscales. However, these procedures are
sample-dependent, with solutions potentially capitalizing on chance or
researchers' degrees of freedom \citep{Simmons2011}. Cross-validation
procedures therefore necessitate data splitting: EFA on one subsample
followed by CFA on another, effectively doubling sample requirements
\citep{Anderson1988}.

\subsection{Designometrics}\label{designometrics}

Psychometrics as a formal theory describes how a set of differing
instruments can produce a combined metric on which to compare the
measured entities. Formally, it should not matter much to construct a
web usability rating scale or a human skill test. Yet, there is an
important difference that defines the \emph{designometric situation}:
First, psychometric measures form a two-way encounter, whereas
comparative design studies has Design as an additional entity, forming a
\emph{response box}. Second, in psychometrics the entity to be ranked is
Person, whereas designometric applications designs are compared.
Whatever role Person samples and parameters play in psychometric
processes, must now be assigned to the Design parameters.

\subsubsection{The designometric
perspective}\label{the-designometric-perspective}

A practical implication of the designometric perspective is that
psychometric tools take flat response matrices as input and are unfit to
process higher-dimensional data. While ``deep'' designometric models can
be constructed using multi-level models \citep[307-323]{Schmettow2021},
a practical solution exists to put psychometric tools to use. By
averaging over Person, a two-dimensional response matrix can be
constructed from a designometric box. This produces a
\emph{designometric response matrix} (design x item), which in turn is
needed to assess the item properties with respect to ranking designs.

For a reliable designometric scale, its items must inter-correlate
strongly, which can only happen when referred-to design features
coincide. Take as an example a hypothetical scale to measure
trustworthiness of robot faces, with two sub-scales, Realism and
Likability. The impression of realism can be triggered by different
features of the face, such as skull shape, details in the eyes region
and skin texture. For a proper scale on realism, it would be required
that these features correlate, and this essentially is a property of the
robot face design process. It is a quite strong assumption that the
effort a robot face designers puts into the eyes region must be strongly
correlated with the effort put into skin texture, but by using
psychometric models with designometric data, assumptions like these can
be tested.

\subsubsection{Designometric scale development}\label{sec-dmx-scale-dev}

Analog to psychometric scale evaluation, substantial samples of designs
are required for item selection and factor identification. This can be a
huge problem, depending on the class of designs. For e-government
websites it will be easier collecting aan adequate sample compared to
human-like robots or self-driving cars.

When a real interactive experience is subject of the measure, a
measurement can take from several minutes to hours and a complete
experimental design with every possible encounter becomes impractical. A
way to mitigate this problem is to use \emph{planned incomplete}
experimental designs. Essentially, a planned incomplete validation study
has all participants encounter only a partition of the design sample.
For example, a sample of 100 designs can be tested by letting every
participant encounter overlapping subsets. As long as all designs are
covered by at least one participant, this will result in a complete
design-by-item matrix after collapsing along participants.

A variation of planned incomplete studies is to \emph{successively}
build the sample of designs. This is especially useful, when dealing
with emerging classes of designs. This happened during the development
of the BUS-11 scale for chatbot evaluation, where initially it was
difficult to build a substantial sample, before large language models
broke through \citep{Borsci2024}.

\subsubsection{The psychometric fallacy}\label{the-psychometric-fallacy}

Designometric scales can be developed with psychometric tools, if using
design x item matrices with sufficiently large sample of designs. In
contrast, many designometric instruments have not been validated using a
large sample of designs, but rather on a psychometric matrix. This we
call the \emph{psychometric fallacy}.

A purportedly designometric instrument that has been validated on a
single or very few designs fell for the \emph{fatal psychometric
fallacy}. In these cases, researchers have failed to recognize a simple
truth: The capacity to discriminate between designs can impossibly be
validated on a single design. Every alleged designometric instrument,
where this has happened during the validation phase, cannot be trusted.

If a substantial sample of designs has been collected, a correct
designometric response matrix can be created by averaging over Persons.
However, the standard terminology in psychometric tools may still
mislead the researcher to believe that producing a psychometric matrix
is correct.

When a scale validation study in design research is under the
psychometric fallacy, validation metrics such as item reliability may be
meaningless for the purpose of ranking designs. Rather, the metric will
refer to the capability of the item to discriminate persons by their
sensitivity to the design feature in question. For example, a scale for
comparing designs by beauty would become a scale to rank persons by how
critical they are with respect to interface aesthetics. This is not the
same and in the next section we show by simulation that the differences
between designometric and psychometric perspectives can be dramatic.

Recall that psychometric validations require large samples of
participants! When swapping roles, validation studies that included
multiple, but only a few, designs, may not be repairable. One example is
the study by \citet{Ho2010} validating the Godspeed Index, a common
multi-scale inventory to evaluate robot designs. Their designometric
study included 38 items and 30 participants, but only 12 designs. While
they did not specify how the designometric box was collapsed, the fact
that they were still able to report exploratory factor analysis results,
suggests that they used a psychometric response matrix, as the
designometric matrix would have been too small to produce stable
estimates. To be fair, the study tested validity correctly comparing
designs, although with simple ANOVA models.

As a milder form \emph{run-time psychometric fallacy} appears when an
existing instrument is used in practice to take measures on a single
design. The result will inevitable look like a psychometric response
matrix and, given that publication rules often require to report test
reliability, it may be tempting for the researcher to apply a
psychometric instrument validation, e.g.~scale reliability. While the
run-time fallacy does not have the same impact as development-time
fallacies, it may cause confusion when a validated instrument seems to
have poor reliability.

\section{Simulation study}\label{simulation-study}

The following example demonstrates the difference by simulating a
situation, where a fictional three-item scale for Coolness is highly
reliable for persons, but seemingly has no reliability at all for
discerning the tested designs. Such a pattern can occur when the design
sample bears little variance with respect to the property in question.
In the following simulation, we assume that the Coolness scale has been
tested on a sample of 20 premium law firm home pages and 20 participants
of various ages and social background.

The simulation uses zero-centered Normal distributions to draw the
parameters for 20 design, 20 participant and 4 items. Subsequently these
are combined into responses \(R = D + P - I\) with some extra noise
(\(\sigma_\textrm{Part} = .5\)). The key here is that items and
participants vary strongly in their appreciation of Coolness
(\(\sigma_\textrm{Part} = \sigma_\textrm{Item} = .2\)), whereas the
sample of designs varies much less in Coolness
(\(\sigma_\textrm{Design} = .05\)).

\begin{longtable}[]{@{}llrrr@{}}

\caption{\label{tbl-simulated-reliability}Reliability under both
perspectives using simulated data (fictional Coolness scale) with 95\%
confidence limits}

\tabularnewline

\toprule\noalign{}
Scale & Perspective & center & lower & upper \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Coolness & designometric & 0.63 & 0.37 & 0.79 \\
Coolness & psychometric & 0.93 & 0.85 & 0.97 \\

\end{longtable}

This simple example demonstrates that psychometric reliability (person
sensitivity) can be excellent (Table~\ref{tbl-simulated-reliability}),
whereas designometric reliability is poor. In the following study we
examine how severe the psychometric fallacy is in real data sets.

\section{Empirical demonstration}\label{empirical-demonstration}

In order to assess the errors introduced by the psychometric fallacy, a
secondary data analysis was conducted using data from seven prior
experiments, which were originally testing hypotheses on User Experience
and Human-Robot Interaction (Table~\ref{tbl-data-summary}). These
studies have in common that data was obtained in complete designometric
encounters, with substantial samples of designs.

\subsection{Methods}\label{methods}

In studies QB, JK, SP and DN (Table~\ref{tbl-data-summary}) participants
saw screen shots of home pages and responded to several user experience
scales \citep[\citet{JK}, \citet{SP}, \citet{DN}]{QB}, whereas in AH, DK
and PS the stimuli were robot faces \citep[\citet{DK}, \citet{PS}]{AH},
with original research questions related to the Uncanny Valley effect.
All experiments used manipulation of presentation time to collect data
on subconscious cognitive processing. For the analysis here,
presentation times lower than 500ms were discarded. All experiments used
a (mildly) incomplete design in that participants encountered all
designs and items several times, but not in every possible combination.

DN used OpenSesame for stimulus presentation and implemented the
collection as graded responses \citep{OpenSesame}. All other experiments
used the same PsychoPy program and collected continuous responses using
a visual analog scale \citep{Peirce2008}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.2400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.2400}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1000}}@{}}

\caption{\label{tbl-data-summary}Summary of data sets used for analysis}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Scale
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Designs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(N_\textrm{Design}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(N_\textrm{Item}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(N_\textrm{Part}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(N_\textrm{Obs}\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SP & Attractiveness & Homepages & 66 & 6 & 40 & 1440 \\
DN & Beauty & Homepages & 48 & 4 & 42 & 2688 \\
JK & Credib & Homepages & 76 & 5 & 25 & 500 \\
QB & HQI & Homepages & 76 & 7 & 25 & 700 \\
QB & HQS & Homepages & 76 & 7 & 25 & 700 \\
DN & Hedonism & Homepages & 48 & 4 & 42 & 2688 \\
DN & Usability & Homepages & 48 & 4 & 42 & 2688 \\
AH & nEeriness & Robot faces & 20 & 8 & 45 & 10800 \\
DK & nEeriness & Robot faces & 80 & 8 & 35 & 2800 \\
PS & nEeriness & Robot faces & 87 & 8 & 39 & 2808 \\

\end{longtable}

In total, eight rating scales were applied to four design samples. The
bipolar \emph{Eeriness} scale is a primary research tool on the Uncanny
Valley phenomenon and measures negative emotional responses towards
artificial faces (see Section~\ref{sec-dmx-scale-dev}). For the purpose
of rendering the Uncanny Valley effect, it has been inverted, hence
\emph{n}Eeriness. Factor structure and reliability were originally
established under a psychometric perspective \citep{Ho2017}. AH used
morphing levels between human and robot faces as stimuli, whereas the
other two experiments used a subset of \citet{Mathur2016}, with a few
new designs added in PS.

All other scales were applied to a sample of commercial home pages
collected by \citet{Tuch2012} (a subset in DN):

\begin{itemize}
\item
  The \emph{Attractiveness} scale is unipolar subscale of the User
  Experience Questionnaire (UEQ) inventory and has undergone basic
  psychometric evaluation in six studies with a single design each
  \citep{Laugwitz2008}.
\item
  The two 7-item bipolar scales \emph{Hedonic Quality - Identity (HQI)}
  and \emph{Hedonic Quality - Stimulation (HQS)} are from the
  AttrakDiff2 inventory, which underwent primary psychometric validation
  on three designs \citep{Hassenzahl2003}. The scales \emph{Hedonism}
  and \emph{Usability} in DN were taken from the short version of
  AttrakDiff2, but will be considered separately in this analysis.
\item
  DN composed the Beauty scale from one item used in
  \citet{Hassenzahl2010} and three items representing classic aesthetics
  from \citet{Tractinsky2006}.
\item
  The \emph{Credibility} scale was originally designed to compare
  people's attitude towards media (newspapers, TV, radio). The
  validation study used exploratory factor analysis for psychometric
  item selection with 1468 participants \citep{Gaziano1986}.
\end{itemize}

Participants were sampled by convenience with sizes between 25 and 45
and a strong over-representation of university-level Social Sciences
students and associated circles.

Goal of the analysis was to examine how the psychometric fallacy
compromises the evaluation of rating scales. For this purpose, each of
the data sets was collapsed into a psychometric and a designometric
response matrix. Subsequently, three basic psychometric techniques were
applied to both perspectives and compared. Scale reliability was
computed using Cronbach's alpha. Item consistency was studied by
inspecting corrected item-total correlations \citep{Psych}.

The number of factors (dimensionality) was identified using parallel
analysis with \texttt{psych::fa.parallel}. This produces an eigenvalue
obtained on real data and compares it to eigenvalues obtained from
simulated. The number of factors is determined as the point before the
real eigenvalue drops below the simulated level.

\subsection{Results}\label{results}

In the following scale, item reliability and factor structures of eight
rating scales are compared by collapsing the item response box along the
design and the participant dimension.

\subsubsection{Scale reliability}\label{scale-reliability}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{DMX_1_files/figure-pdf/fig-scale-reliability-1.pdf}}

}

\caption{\label{fig-scale-reliability}Cronbach alpha item-level
reliability estimates compared by perspective and scale}

\end{figure}%

Overall scale reliabilities cover a broad range from excellent to
unusable Figure~\ref{fig-scale-reliability}. All scales perform better
under the designometric perspective, albeit, the difference ranges from
barely noticable (HQS, HQI) to very strong (Hedonism, Usability, Beauty
and Attractiveness). The most dramatic difference can be seen in
Hedonism and Beauty, which both have excellent designometric
reliability, but poor reliability from a psychometric perspective.

\subsubsection{Item consistency}\label{item-consistency}

Figure~\ref{fig-item-reliability} shows corrected item-total
correlations reflecting item consistency. Beauty and Hedonism stand out,
because all items show higher designometric item reliability. To some
extent this also seems to hold for Usability and Eeriness. For
Credibility, HQ-I, HQ-S and Attractiveness some items perform poorer
under the psychometric perspective, whereas others improve, with one
extreme cases: Item Att6 is already on a very low level on designometric
performance and is showing even a negatively correlation under the
psychometric perspective. Items HQI5 and HQI6 show poor designometric
performance, but are among the overall best performing psychometric
items.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{DMX_1_files/figure-pdf/fig-item-reliability-1.pdf}}

}

\caption{\label{fig-item-reliability}Cronbach alpha item-level
reliability estimates compared by perspective and scale. Scale
reliability repeated in Red}

\end{figure}%

\subsubsection{Number of factors}\label{number-of-factors}

Given a response matrix, the number of factors were estimated using
parallel analysis. Ideally, this procedure returns exactly as many
factors as there are separate scales in every data set.

The Eeriness scale is part of a larger Godspeed Index inventory and is
supposed to represent a single latent variable. However, \citet{Ho2017}
found that the scale decomposes into two slightly different aspects,
summarized as ``eerie'' and ``spine-tingling''. This was established
using principal component analysis, whereas a dedicated identification
of the number of factors has not been reported. Since this was tested
with only 12 designs, most likely it was under psychometric perspective.
The results in Figure~\ref{fig-nfactors-1} show that for both
perspectives the eigenvalue drops below the simulated eigenvalue with
two factors under both perspectives.

On theoretical grounds, the AttrakDiff2 inventory splits hedonistic
quality into two components, Identity and Stimulation, while the
credibility scale is a completely separate construct. We would expect
three factors to emerge. Figure~\ref{fig-nfactors-2} shows that the two
perspectives deviate in opposite directions: for the psychometric
perspective, the eigenvalues drop below their simulated counterparts at
two factors, whereas for the designometric perspective stays above this
line with five factors.

Finally, in study DN three independent scales, Hedonism, Usability and
Beauty, were used und we expect three factors. In contrast, the
eigenvalues drop below even the simulated eigenvalues with two factors,
suggesting that the same latent variable is captured by all three scales
under both perspectives (Figure~\ref{fig-nfactors-3}).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{DMX_1_files/figure-pdf/fig-nfactors-1.pdf}}

}

\subcaption{\label{fig-nfactors-1}Eeriness scale}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{DMX_1_files/figure-pdf/fig-nfactors-2.pdf}}

}

\subcaption{\label{fig-nfactors-2}AttrakDiff and Credibility}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{DMX_1_files/figure-pdf/fig-nfactors-3.pdf}}

}

\subcaption{\label{fig-nfactors-3}Hedonism, Usability and Beauty}

\end{minipage}%

\caption{\label{fig-nfactors}Parallel analysis results for
identification of the number of factors under designometric and
psychometric perspectives.}

\end{figure}%

\section{Discussion}\label{discussion}

Rating scales in Human Factors research are commonly used to
discriminate between poor and good design options, rank designs, choose
designs, or perform UX regression tests in continuous design cycles. Our
logical argument is that the capability of a scale to rank designs can
only be seen on multiple designs, hence using design-by-item response
matrices. We called it the psychometric fallacy to use person-by-item
response matrices in place. A simulation showed, that the worst case can
happen under the psychometric fallacy: excellent reliability is
reported, when it actually is poor.

With empirical data from five experiments we showed that the
psychometric fallacy has real-world implicitations and produces wrong
interpretations across the board, sometimes dramatic. In many cases,
scale reliability is very different between the two perspectives, with
designometric reliability being generally higher. While the differences
in reliability were large across scales, on designometric level they
were all in a useful range, ranging from just useful (Credibility) to
excellent (Hedonism, Beauty). But, many differences exist on item level,
going in both directions. Accordingly, the factor numbers differ from
theoretical expectations in all but one cases (Eeriness).

In the following we discuss the details and implications of our findings
for scale developers and users, before we outline an agenda for more
advanced (deep) designometric methods.

\subsection{Implications for scale
development}\label{implications-for-scale-development}

In design research the target of all research is quickly changing and
expanding. A certain swiftness and pragmatism is required to keep up
with the pace. Development of new scales is a common task, and often it
is carried out by researchers with a basic understanding of psychometric
principles, such as (item) reliability and exploratory factor analysis.

Basic psychometric tools produce vastly different results under the
psychometric fallacy. While our study used mature scales, which had
already undergone item selection and perhaps factor analysis, we can
interpolate the consequences for future scale development.

The most severe consequence is that a scale may be developed that is not
capable of ranking designs. According to an often cited rule-of-thumb,
scale reliability should be at least .7. Three scales in our study,
Attractiveness, Credibility and HQ-Stimulation did not meet this
criterion, even under the designometric perspective.

The HQ-Identity scale (and to some extent also Credibility) shows a
concerning pattern, where some items perform well psychometrically, but
are designometrically extremely weak. This shows that developing a
designometric scale under psychometric perspective can lead to
\emph{falsely favored} items that are well-behaved in ranking persons,
but are inefficient for designs. To make the case, scale reliability
when removing item HQI5 improves to \(0.73\) compared to \(0.69\). By
further removing HQI1, reliability is \(0.68\). While this is not
strictly an increase in reliability, the same level is effectively
reached with fewer items - the psychometric fallacy can lead to
inefficient scales.

While we cannot show that directly, it is likely that fallacy also leads
to \emph{falsely rejected} items that are actually well-behaved in
ranking designs. Creating an item pool is by itself a time-consuming
process, and the psychometric fallacy can make it even more difficult by
rejecting good items and selecting inefficient ones. A recent example is
the development of the BUS-11 scale, where face validity demands (and
factor analysis has confirmed) that \emph{Privacy} is a separate
construct, but only one item was left after item selection under
psychometric perspective \citep{Borsci2022}.

\subsection{Implications for users}\label{implications-for-users}

For practitioners, the good news is that if they were under the run-time
psychometric fallacy by routinely reporting scale reliability, they were
probably better than they said. And when they continue to use these
scales in the future, the improved precision will allow them to reduce
sample sizes.

But, practitioners may not yet have the most efficient rating scales.
Even if a false favored item is not directly harming reliability, it can
make the scale inefficient. In practice, UX scales are often deployed
during use, for example in usability tests. Shorter scales can be
applied in quicker succession, for example once per task, or everyday in
a longitudinal study. It is therefore not uncommon for practitioners to
create a reduced scale, for example, when many latent variables are
involved. For some scales (Hedonism, Beauty) it is safe to just pick
three items at random. Other scales are quite mixed bags, with the
highest ranked item under psychometric perspective being the lowest
ranked designometrically.

\subsection{Applications}\label{applications}

Emerging technologies are often characterized by an innovation phase,
where multiple design paths are explored in a rush to the market.
Several domains of human-technology interaction are currently gaining
momentum: large language model technology is, as of writing, receiving
much attention for intelligent agent design. Humanoid and animalistic
robot design is coming out of its niche, and virtual reality
applications are on the verge to mainstream. These three domains have in
common that, compared to classic computer applications, they are tapping
into new territories of the users mind, the social mind and the
sensation of physical reality.

When Social Experience (SX) or Virtual Experience (VX) become the new
UX, it may start with the same abundance of new instruments trying to
map the uncharted design space. This space is huge and effective
designometric instruments are needed to guide critical design decisions
in hyper-excited times \citep{hypecycle2023}. Our results show, that the
psychometric fallacy is harmful during scale development, leading to
inefficient item sets and factor structures. It must not be purported.

\subsection{Towards Deep
Designometrics}\label{towards-deep-designometrics}

By comparing the two perspectives, we illustrated that designometrics
can be accomplished with standard psychometric tools by flattening the
response box across participants. In principle, averaging across users
is legit, except in situations where users did not evaluate the same set
of designs. But even when all users were evaluating the same set of
designs, this averaging across users results in information loss. More
specifically we lose information about the users, which would be
interesting in its own right. As an extreme case, it is possible to
evaluate a designometric model on the basis of a response matrix
collected from a single user. Formally, this would be a valid
designometric measure, reflecting a single person's ranking of designs.

Designometric scales are commonly used to measure a populations reaction
to a design, which implies that on some level the psychometric matrix is
useful, for example to study the distribution of user sensitivity to a
feature. Imaginable cases exist where one could use a designometric
scale for psychometric purposes. For example, an instrument to measure
trustworthiness of designs could be used to estimate faithfulness of
participants in a study (or a training) on cyber security.

By flattening the designometric box one way, then the other, we still
loose information that is needed to secure that items are truly
well-behaved. In educational psychometrics \emph{differential item
functioning} is the idea that items must be fair and function the same
for every tested person. This is a desirable property for a
designometric scale, but a statistical model for verification would need
individual parameters for participants, designs and items,
simultaneously. \citet{Schmettow2021_multilevel} proposed multi-level
models for capturing designometric situations in their full dimension,
which could be well-suited for run-time use or basic scale development.

On a broader scale the multi-level approach is well in line with the
established field of generalizability theory \citep{Brennan2001}. In
that approach, the variance in responses is partitioned into multiple
sources. Thus, suppose we have a data collection design where multiple
users judge several designs, on multiple items, the variance in
responses can be partitioned into variance due to individual differences
in users, differences in designs, and differences in items. This can be
done for the response box, but can straightforwardly be extended to
hyperboxes. The designometric encounter may not be end of story. For
example, for comparing multi-purpose designs a researcher may want to
add tasks as fourth population of interest \citep{Schmettow2013s}.

Such variance decomposition can be implemented in multi-level models,
where the estimated variances can be used to compute reliability,
whether it relates to measuring differences in users or differences in
designs, while controlling for all other relevant sources of variation.
By extending the modelling to multi-level IRT models
\citep[\citet{vandenBerg2007}]{Fox2001} the discrete nature of the item
response data can be taken into account.

Multi-dimensional exploratory methods have been well developed in
chemometrics and sensory science
\citep[\citet{Harshman1970}]{Tucker1966}, but have seen little
integration into mainstream psychometric or UX-scale validation
workflows. Bridging this gap---by extending factor-analytic methods to
multi-dimensional designometric data---constitutes a critical next step
in establishing a rigorous quantitative foundation for multi-factor
measurements.

Finally, the Eeriness scale was shown to robustly perform under both
perspectives, proving that \emph{universal rating scales} are possible.
Understanding how to create and validate such scales is an important
next step in developing a deeper designometric methodology.

\section{AI Statement}\label{ai-statement}

During the preparation of this work the authors used ChatGPT in order to
improve language and structure. After using this service, the authors
reviewed and edited the content as needed and take full responsibility
for the content of the published article.


\renewcommand\refname{References}
\bibliography{dmx.bib}



\end{document}
